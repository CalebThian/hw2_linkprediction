{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "84bea4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.datasets import Planetoid, Coauthor, Amazon\n",
    "from torch_geometric.utils import train_test_split_edges, dense_to_sparse\n",
    "from torch_geometric.nn import GAE, VGAE, APPNP\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ec2ac122",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E370</td>\n",
       "      <td>26</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E667</td>\n",
       "      <td>196</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3190</td>\n",
       "      <td>739</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E848</td>\n",
       "      <td>576</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E2161</td>\n",
       "      <td>466</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   to  from\n",
       "0   E370   26   317\n",
       "1   E667  196   323\n",
       "2  E3190  739   468\n",
       "3   E848  576   156\n",
       "4  E2161  466   199"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"hw2_data/\"\n",
    "dss = ['dataset1','dataset2','dataset3'] #datasets\n",
    "datasets = dict()\n",
    "for ds in dss:\n",
    "    datasets[ds] = dict()\n",
    "    datasets[ds]['content'] = pd.read_csv(path+ds+\"/content.csv\",delimiter = '\\t',header = None)\n",
    "    datasets[ds]['train'] = pd.read_csv(path+ds+\"/train.csv\",delimiter = ',')\n",
    "    datasets[ds]['test'] = pd.read_csv(path+ds+\"/test.csv\",delimiter = ',')\n",
    "    datasets[ds]['upload'] = pd.read_csv(path+ds+\"/upload.csv\",delimiter = ',')\n",
    "datasets[dss[2]]['test'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "869cd0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Cora dataset\n",
    "dataset = Planetoid(root='/tmp/cora', name='Cora')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "3f39f155",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset[0]\n",
    "data = T.NormalizeFeatures()(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "5e040547",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_rate = 0.85\n",
    "val_ratio = (1-train_rate) / 3\n",
    "test_ratio = (1-train_rate) / 3 * 2\n",
    "data = train_test_split_edges(data.to(dev), val_ratio=val_ratio, test_ratio=test_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "acff22da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example:\n",
    "dss_num = 1\n",
    "Content = datasets[dss[dss_num]]['content']\n",
    "Train = datasets[dss[dss_num]]['train']\n",
    "Test = datasets[dss[dss_num]]['test']\n",
    "Upload = datasets[dss[dss_num]]['upload']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "0190f679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E5138</td>\n",
       "      <td>1747</td>\n",
       "      <td>3160</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E4502</td>\n",
       "      <td>2879</td>\n",
       "      <td>2138</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E365</td>\n",
       "      <td>1051</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>E6881</td>\n",
       "      <td>1976</td>\n",
       "      <td>1071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>E2263</td>\n",
       "      <td>1674</td>\n",
       "      <td>2032</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7536</th>\n",
       "      <td>E1115</td>\n",
       "      <td>2515</td>\n",
       "      <td>2359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7537</th>\n",
       "      <td>E7112</td>\n",
       "      <td>1056</td>\n",
       "      <td>1325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7538</th>\n",
       "      <td>E1399</td>\n",
       "      <td>740</td>\n",
       "      <td>115</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7541</th>\n",
       "      <td>E9426</td>\n",
       "      <td>2958</td>\n",
       "      <td>2884</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7542</th>\n",
       "      <td>E2512</td>\n",
       "      <td>2818</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3736 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id    to  from  label\n",
       "0     E5138  1747  3160      1\n",
       "3     E4502  2879  2138      1\n",
       "6      E365  1051  1030      1\n",
       "8     E6881  1976  1071      1\n",
       "13    E2263  1674  2032      1\n",
       "...     ...   ...   ...    ...\n",
       "7536  E1115  2515  2359      1\n",
       "7537  E7112  1056  1325      1\n",
       "7538  E1399   740   115      1\n",
       "7541  E9426  2958  2884      1\n",
       "7542  E2512  2818  2024      1\n",
       "\n",
       "[3736 rows x 4 columns]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train[Train['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "23b5385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content,train, test):\n",
    "    G = nx.DiGraph()\n",
    "    G_test = nx.DiGraph()\n",
    "    # for easier split the edges, create 2 graph, 1 with positive edge, the other with given negative edges\n",
    "    G_pos = nx.DiGraph()\n",
    "    G_neg = nx.DiGraph()\n",
    "    graph_node_features_dict = dict()\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        #graph_node_features_dict[content.iloc[i,0]] = np.array(content.iloc[i,1:])\n",
    "        G.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        G_test.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        # pos and neg\n",
    "        G_pos.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        G_neg.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        \n",
    "    for i in range(len(train)):\n",
    "        # Adding nodes into G\n",
    "        '''\n",
    "        if train.loc[i,'from'] not in G:\n",
    "            G.add_node(train.loc[i,'from'],features = graph_node_features_dict[train.loc[i,'from']])\n",
    "        if train.loc[i,'to'] not in G:\n",
    "            G.add_node(train.loc[i,'to'],features = graph_node_features_dict[train.loc[i,'to']])\n",
    "        ''' \n",
    "        # Adding edges\n",
    "        G.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "        \n",
    "        # pos and neg\n",
    "        if train.loc[i,'label'] == 0: \n",
    "            G_neg.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "        else:\n",
    "            G_pos.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        # Adding edges\n",
    "        G_test.add_edge(test.loc[i,'from'],test.loc[i,'to'])\n",
    "    \n",
    "    adj = nx.adjacency_matrix(G,sorted(G.nodes()))\n",
    "    adj_test = nx.adjacency_matrix(G_test,G_test.nodes())\n",
    "    adj_pos = nx.adjacency_matrix(G_pos,sorted(G_pos.nodes()))\n",
    "    adj_neg = nx.adjacency_matrix(G_neg,sorted(G_neg.nodes()))\n",
    "    features = np.array(\n",
    "        [features for _, features in sorted(G.nodes(data='features'), key=lambda x: x[0])])\n",
    "    \n",
    "    # Skip train,valid,and test mask\n",
    "    \n",
    "    num_features = features.shape[1]\n",
    "    features = torch.FloatTensor(features)\n",
    "    return adj, adj_test, adj_pos, adj_neg, features, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "6d2ed60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, gtest, pos, neg, features, num_features = load_data(Content,Train,Test)\n",
    "A = g.toarray()\n",
    "edge_index,_ = dense_to_sparse(torch.tensor(A))\n",
    "Atest = gtest.toarray()\n",
    "edge_index_test,_ = dense_to_sparse(torch.tensor(Atest))\n",
    "Apos = pos.toarray()\n",
    "edge_index_pos,_ = dense_to_sparse(torch.tensor(Apos))\n",
    "Aneg = neg.toarray()\n",
    "edge_index_neg,_ = dense_to_sparse(torch.tensor(Aneg))\n",
    "data = Data(edge_index=edge_index_pos,x=features.to(torch.float),test = torch.tensor([Test['from'],Test['to']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "b30e0a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch_geometric\\deprecation.py:12: UserWarning: 'train_test_split_edges' is deprecated, use 'transforms.RandomLinkSplit' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Data(x=[3312, 3703], test=[2, 1886], val_pos_edge_index=[2, 177], test_pos_edge_index=[2, 0], train_pos_edge_index=[2, 3202], train_neg_adj_mask=[3312, 3312], val_neg_edge_index=[2, 177], test_neg_edge_index=[2, 0])"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_rate = 0.9\n",
    "val_ratio = (1-train_rate)#/3\n",
    "test_ratio = 0#(1-train_rate) / 3 * 2\n",
    "data = train_test_split_edges(data.to(dev), val_ratio=val_ratio, test_ratio=0)\n",
    "               #,train_pos_edge_index = edge_index_pos,neg = edge_index_neg,test = edge_index_test)\n",
    "data = T.NormalizeFeatures()(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "6e3deb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_used = 'VGNAE'\n",
    "scaling_factor = 1.8\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, edge_index):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(in_channels, out_channels)\n",
    "        self.linear2 = nn.Linear(in_channels, out_channels)\n",
    "        self.propagate = APPNP(K=1, alpha=0)\n",
    "\n",
    "    def forward(self, x, edge_index,not_prop=0):\n",
    "        if model_used  == 'GNAE':\n",
    "            x = self.linear1(x)\n",
    "            x = F.normalize(x,p=2,dim=1)  * scaling_factor\n",
    "            x = self.propagate(x, edge_index)\n",
    "            return x\n",
    "\n",
    "        if model_used  == 'VGNAE':\n",
    "            x_ = self.linear1(x)\n",
    "            x_ = self.propagate(x_, edge_index)\n",
    "\n",
    "            x = self.linear2(x)\n",
    "            x = F.normalize(x,p=2,dim=1) * scaling_factor\n",
    "            x = self.propagate(x, edge_index)\n",
    "            return x, x_\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "3c0f6549",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "channels = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "22b6f0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = int(data.x.size()[0])\n",
    "model_used = 'VGNAE'\n",
    "if model_used  == 'GNAE':   \n",
    "    model = GAE(Encoder(data.x.size()[1], channels, data.train_pos_edge_index)).to(dev)\n",
    "if model_used  == 'VGNAE':\n",
    "    model = VGAE(Encoder(data.x.size()[1], channels, data.train_pos_edge_index)).to(dev)\n",
    "\n",
    "data.train_mask = data.val_mask = data.test_mask = data.y = None\n",
    "x, train_pos_edge_index = data.x.to(dev), data.train_pos_edge_index.to(dev)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "13e3f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    z  = model.encode(x, train_pos_edge_index)\n",
    "    loss = model.recon_loss(z, train_pos_edge_index)\n",
    "    if model_used  in ['VGNAE']:\n",
    "        loss = loss + (1 / data.num_nodes) * model.kl_loss()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "1c226a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(pos_edge_index, neg_edge_index):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model.encode(x, train_pos_edge_index)\n",
    "    return model.test(z, pos_edge_index, neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "3a89198b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6760509432155511, 0.7106720869789769)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(data.val_pos_edge_index, data.val_neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "16169cac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, LOSS: 15.0006, AUC: 0.6856, AP: 0.7187\n",
      "Epoch: 002, LOSS: 14.9991, AUC: 0.6930, AP: 0.7253\n",
      "Epoch: 003, LOSS: 14.6225, AUC: 0.6953, AP: 0.7273\n",
      "Epoch: 004, LOSS: 14.0947, AUC: 0.6969, AP: 0.7284\n",
      "Epoch: 005, LOSS: 14.2779, AUC: 0.6979, AP: 0.7291\n",
      "Epoch: 006, LOSS: 14.1817, AUC: 0.6991, AP: 0.7300\n",
      "Epoch: 007, LOSS: 14.2699, AUC: 0.6997, AP: 0.7303\n",
      "Epoch: 008, LOSS: 13.8928, AUC: 0.7005, AP: 0.7309\n",
      "Epoch: 009, LOSS: 13.5797, AUC: 0.7013, AP: 0.7315\n",
      "Epoch: 010, LOSS: 13.2887, AUC: 0.7020, AP: 0.7321\n",
      "Epoch: 011, LOSS: 13.6428, AUC: 0.7027, AP: 0.7326\n",
      "Epoch: 012, LOSS: 13.5392, AUC: 0.7034, AP: 0.7332\n",
      "Epoch: 013, LOSS: 12.7460, AUC: 0.7040, AP: 0.7336\n",
      "Epoch: 014, LOSS: 12.9277, AUC: 0.7047, AP: 0.7340\n",
      "Epoch: 015, LOSS: 13.0611, AUC: 0.7053, AP: 0.7344\n",
      "Epoch: 016, LOSS: 12.3376, AUC: 0.7057, AP: 0.7346\n",
      "Epoch: 017, LOSS: 11.6822, AUC: 0.7062, AP: 0.7349\n",
      "Epoch: 018, LOSS: 12.1587, AUC: 0.7071, AP: 0.7354\n",
      "Epoch: 019, LOSS: 11.6547, AUC: 0.7077, AP: 0.7359\n",
      "Epoch: 020, LOSS: 11.5789, AUC: 0.7083, AP: 0.7362\n",
      "Epoch: 021, LOSS: 11.6349, AUC: 0.7090, AP: 0.7366\n",
      "Epoch: 022, LOSS: 11.3415, AUC: 0.7098, AP: 0.7371\n",
      "Epoch: 023, LOSS: 11.2554, AUC: 0.7106, AP: 0.7376\n",
      "Epoch: 024, LOSS: 11.5809, AUC: 0.7113, AP: 0.7380\n",
      "Epoch: 025, LOSS: 10.8511, AUC: 0.7122, AP: 0.7385\n",
      "Epoch: 026, LOSS: 11.0907, AUC: 0.7131, AP: 0.7390\n",
      "Epoch: 027, LOSS: 11.2177, AUC: 0.7143, AP: 0.7398\n",
      "Epoch: 028, LOSS: 10.5013, AUC: 0.7157, AP: 0.7410\n",
      "Epoch: 029, LOSS: 10.7781, AUC: 0.7171, AP: 0.7420\n",
      "Epoch: 030, LOSS: 10.6591, AUC: 0.7190, AP: 0.7433\n",
      "Epoch: 031, LOSS: 10.5984, AUC: 0.7215, AP: 0.7451\n",
      "Epoch: 032, LOSS: 10.1882, AUC: 0.7234, AP: 0.7464\n",
      "Epoch: 033, LOSS: 10.1612, AUC: 0.7260, AP: 0.7481\n",
      "Epoch: 034, LOSS: 9.9098, AUC: 0.7286, AP: 0.7497\n",
      "Epoch: 035, LOSS: 10.1972, AUC: 0.7319, AP: 0.7516\n",
      "Epoch: 036, LOSS: 9.7758, AUC: 0.7351, AP: 0.7536\n",
      "Epoch: 037, LOSS: 9.8374, AUC: 0.7396, AP: 0.7567\n",
      "Epoch: 038, LOSS: 9.5677, AUC: 0.7439, AP: 0.7595\n",
      "Epoch: 039, LOSS: 9.4022, AUC: 0.7484, AP: 0.7627\n",
      "Epoch: 040, LOSS: 9.2204, AUC: 0.7543, AP: 0.7669\n",
      "Epoch: 041, LOSS: 9.3707, AUC: 0.7603, AP: 0.7713\n",
      "Epoch: 042, LOSS: 9.3746, AUC: 0.7682, AP: 0.7783\n",
      "Epoch: 043, LOSS: 9.2863, AUC: 0.7772, AP: 0.7852\n",
      "Epoch: 044, LOSS: 8.7450, AUC: 0.7896, AP: 0.7961\n",
      "Epoch: 045, LOSS: 8.5239, AUC: 0.8041, AP: 0.8077\n",
      "Epoch: 046, LOSS: 8.3991, AUC: 0.8204, AP: 0.8207\n",
      "Epoch: 047, LOSS: 8.2122, AUC: 0.8397, AP: 0.8360\n",
      "Epoch: 048, LOSS: 8.0145, AUC: 0.8626, AP: 0.8528\n",
      "Epoch: 049, LOSS: 7.8864, AUC: 0.8893, AP: 0.8733\n",
      "Epoch: 050, LOSS: 7.8185, AUC: 0.9163, AP: 0.9003\n",
      "Epoch: 051, LOSS: 7.1171, AUC: 0.9377, AP: 0.9253\n",
      "Epoch: 052, LOSS: 7.0879, AUC: 0.9482, AP: 0.9402\n",
      "Epoch: 053, LOSS: 6.8853, AUC: 0.9515, AP: 0.9456\n",
      "Epoch: 054, LOSS: 6.4485, AUC: 0.9514, AP: 0.9460\n",
      "Epoch: 055, LOSS: 6.5275, AUC: 0.9488, AP: 0.9430\n",
      "Epoch: 056, LOSS: 6.2409, AUC: 0.9460, AP: 0.9398\n",
      "Epoch: 057, LOSS: 6.2338, AUC: 0.9453, AP: 0.9393\n",
      "Epoch: 058, LOSS: 6.2754, AUC: 0.9460, AP: 0.9399\n",
      "Epoch: 059, LOSS: 6.0232, AUC: 0.9466, AP: 0.9399\n",
      "Epoch: 060, LOSS: 5.9768, AUC: 0.9471, AP: 0.9410\n",
      "Epoch: 061, LOSS: 5.8466, AUC: 0.9461, AP: 0.9394\n",
      "Epoch: 062, LOSS: 5.6824, AUC: 0.9453, AP: 0.9382\n",
      "Epoch: 063, LOSS: 5.2199, AUC: 0.9454, AP: 0.9381\n",
      "Epoch: 064, LOSS: 5.5327, AUC: 0.9458, AP: 0.9378\n",
      "Epoch: 065, LOSS: 5.5143, AUC: 0.9466, AP: 0.9382\n",
      "Epoch: 066, LOSS: 5.7700, AUC: 0.9471, AP: 0.9384\n",
      "Epoch: 067, LOSS: 5.2913, AUC: 0.9470, AP: 0.9383\n",
      "Epoch: 068, LOSS: 5.3113, AUC: 0.9470, AP: 0.9387\n",
      "Epoch: 069, LOSS: 5.1508, AUC: 0.9465, AP: 0.9384\n",
      "Epoch: 070, LOSS: 5.0973, AUC: 0.9460, AP: 0.9381\n",
      "Epoch: 071, LOSS: 4.9545, AUC: 0.9454, AP: 0.9375\n",
      "Epoch: 072, LOSS: 4.8088, AUC: 0.9447, AP: 0.9370\n",
      "Epoch: 073, LOSS: 4.8295, AUC: 0.9437, AP: 0.9362\n",
      "Epoch: 074, LOSS: 4.9238, AUC: 0.9431, AP: 0.9358\n",
      "Epoch: 075, LOSS: 4.6156, AUC: 0.9426, AP: 0.9354\n",
      "Epoch: 076, LOSS: 4.7194, AUC: 0.9419, AP: 0.9349\n",
      "Epoch: 077, LOSS: 4.5360, AUC: 0.9412, AP: 0.9342\n",
      "Epoch: 078, LOSS: 4.3852, AUC: 0.9407, AP: 0.9336\n",
      "Epoch: 079, LOSS: 4.4616, AUC: 0.9399, AP: 0.9330\n",
      "Epoch: 080, LOSS: 4.4334, AUC: 0.9394, AP: 0.9326\n",
      "Epoch: 081, LOSS: 4.3947, AUC: 0.9398, AP: 0.9327\n",
      "Epoch: 082, LOSS: 4.2809, AUC: 0.9397, AP: 0.9322\n",
      "Epoch: 083, LOSS: 4.1778, AUC: 0.9399, AP: 0.9319\n",
      "Epoch: 084, LOSS: 4.0524, AUC: 0.9404, AP: 0.9326\n",
      "Epoch: 085, LOSS: 4.0548, AUC: 0.9404, AP: 0.9327\n",
      "Epoch: 086, LOSS: 3.8669, AUC: 0.9403, AP: 0.9326\n",
      "Epoch: 087, LOSS: 4.0826, AUC: 0.9407, AP: 0.9327\n",
      "Epoch: 088, LOSS: 4.0025, AUC: 0.9410, AP: 0.9328\n",
      "Epoch: 089, LOSS: 3.8590, AUC: 0.9408, AP: 0.9326\n",
      "Epoch: 090, LOSS: 3.8068, AUC: 0.9410, AP: 0.9328\n",
      "Epoch: 091, LOSS: 3.9562, AUC: 0.9408, AP: 0.9326\n",
      "Epoch: 092, LOSS: 3.7075, AUC: 0.9411, AP: 0.9325\n",
      "Epoch: 093, LOSS: 3.8267, AUC: 0.9408, AP: 0.9313\n",
      "Epoch: 094, LOSS: 3.7355, AUC: 0.9407, AP: 0.9314\n",
      "Epoch: 095, LOSS: 3.5362, AUC: 0.9401, AP: 0.9311\n",
      "Epoch: 096, LOSS: 3.5897, AUC: 0.9399, AP: 0.9311\n",
      "Epoch: 097, LOSS: 3.7028, AUC: 0.9397, AP: 0.9311\n",
      "Epoch: 098, LOSS: 3.5061, AUC: 0.9391, AP: 0.9306\n",
      "Epoch: 099, LOSS: 3.5281, AUC: 0.9387, AP: 0.9305\n",
      "Epoch: 100, LOSS: 3.4405, AUC: 0.9386, AP: 0.9309\n",
      "Epoch: 101, LOSS: 3.3081, AUC: 0.9379, AP: 0.9303\n",
      "Epoch: 102, LOSS: 3.2756, AUC: 0.9380, AP: 0.9303\n",
      "Epoch: 103, LOSS: 3.4507, AUC: 0.9384, AP: 0.9309\n",
      "Epoch: 104, LOSS: 3.2356, AUC: 0.9386, AP: 0.9309\n",
      "Epoch: 105, LOSS: 3.3958, AUC: 0.9384, AP: 0.9306\n",
      "Epoch: 106, LOSS: 3.1874, AUC: 0.9385, AP: 0.9308\n",
      "Epoch: 107, LOSS: 3.2911, AUC: 0.9386, AP: 0.9307\n",
      "Epoch: 108, LOSS: 3.1389, AUC: 0.9388, AP: 0.9300\n",
      "Epoch: 109, LOSS: 3.1148, AUC: 0.9393, AP: 0.9305\n",
      "Epoch: 110, LOSS: 3.1520, AUC: 0.9399, AP: 0.9312\n",
      "Epoch: 111, LOSS: 2.9597, AUC: 0.9402, AP: 0.9316\n",
      "Epoch: 112, LOSS: 3.1012, AUC: 0.9402, AP: 0.9316\n",
      "Epoch: 113, LOSS: 3.0954, AUC: 0.9403, AP: 0.9317\n",
      "Epoch: 114, LOSS: 3.0178, AUC: 0.9400, AP: 0.9315\n",
      "Epoch: 115, LOSS: 2.9906, AUC: 0.9399, AP: 0.9315\n",
      "Epoch: 116, LOSS: 2.9512, AUC: 0.9394, AP: 0.9311\n",
      "Epoch: 117, LOSS: 3.0022, AUC: 0.9390, AP: 0.9309\n",
      "Epoch: 118, LOSS: 2.7976, AUC: 0.9382, AP: 0.9301\n",
      "Epoch: 119, LOSS: 2.9091, AUC: 0.9379, AP: 0.9299\n",
      "Epoch: 120, LOSS: 2.8725, AUC: 0.9376, AP: 0.9296\n",
      "Epoch: 121, LOSS: 2.7512, AUC: 0.9377, AP: 0.9297\n",
      "Epoch: 122, LOSS: 2.8139, AUC: 0.9379, AP: 0.9298\n",
      "Epoch: 123, LOSS: 2.8173, AUC: 0.9381, AP: 0.9300\n",
      "Epoch: 124, LOSS: 2.6634, AUC: 0.9382, AP: 0.9299\n",
      "Epoch: 125, LOSS: 2.5243, AUC: 0.9385, AP: 0.9300\n",
      "Epoch: 126, LOSS: 2.6474, AUC: 0.9387, AP: 0.9303\n",
      "Epoch: 127, LOSS: 2.7555, AUC: 0.9390, AP: 0.9302\n",
      "Epoch: 128, LOSS: 2.8427, AUC: 0.9393, AP: 0.9303\n",
      "Epoch: 129, LOSS: 2.7108, AUC: 0.9397, AP: 0.9308\n",
      "Epoch: 130, LOSS: 2.6809, AUC: 0.9394, AP: 0.9310\n",
      "Epoch: 131, LOSS: 2.6267, AUC: 0.9392, AP: 0.9309\n",
      "Epoch: 132, LOSS: 2.6055, AUC: 0.9394, AP: 0.9310\n",
      "Epoch: 133, LOSS: 2.5718, AUC: 0.9392, AP: 0.9310\n",
      "Epoch: 134, LOSS: 2.4742, AUC: 0.9392, AP: 0.9311\n",
      "Epoch: 135, LOSS: 2.5595, AUC: 0.9388, AP: 0.9311\n",
      "Epoch: 136, LOSS: 2.4725, AUC: 0.9385, AP: 0.9309\n",
      "Epoch: 137, LOSS: 2.4389, AUC: 0.9382, AP: 0.9310\n",
      "Epoch: 138, LOSS: 2.5341, AUC: 0.9384, AP: 0.9314\n",
      "Epoch: 139, LOSS: 2.5314, AUC: 0.9386, AP: 0.9318\n",
      "Epoch: 140, LOSS: 2.4063, AUC: 0.9389, AP: 0.9318\n",
      "Epoch: 141, LOSS: 2.4682, AUC: 0.9389, AP: 0.9318\n",
      "Epoch: 142, LOSS: 2.3488, AUC: 0.9389, AP: 0.9317\n",
      "Epoch: 143, LOSS: 2.3416, AUC: 0.9393, AP: 0.9319\n",
      "Epoch: 144, LOSS: 2.4210, AUC: 0.9390, AP: 0.9315\n",
      "Epoch: 145, LOSS: 2.3806, AUC: 0.9390, AP: 0.9317\n",
      "Epoch: 146, LOSS: 2.3433, AUC: 0.9389, AP: 0.9315\n",
      "Epoch: 147, LOSS: 2.3360, AUC: 0.9392, AP: 0.9314\n",
      "Epoch: 148, LOSS: 2.3656, AUC: 0.9395, AP: 0.9319\n",
      "Epoch: 149, LOSS: 2.2681, AUC: 0.9394, AP: 0.9318\n",
      "Epoch: 150, LOSS: 2.2774, AUC: 0.9396, AP: 0.9321\n",
      "Epoch: 151, LOSS: 2.3106, AUC: 0.9396, AP: 0.9322\n",
      "Epoch: 152, LOSS: 2.2871, AUC: 0.9397, AP: 0.9323\n",
      "Epoch: 153, LOSS: 2.2420, AUC: 0.9396, AP: 0.9324\n",
      "Epoch: 154, LOSS: 2.2761, AUC: 0.9396, AP: 0.9324\n",
      "Epoch: 155, LOSS: 2.1115, AUC: 0.9398, AP: 0.9328\n",
      "Epoch: 156, LOSS: 2.1376, AUC: 0.9401, AP: 0.9328\n",
      "Epoch: 157, LOSS: 2.2151, AUC: 0.9402, AP: 0.9331\n",
      "Epoch: 158, LOSS: 2.2151, AUC: 0.9406, AP: 0.9337\n",
      "Epoch: 159, LOSS: 2.0781, AUC: 0.9406, AP: 0.9335\n",
      "Epoch: 160, LOSS: 2.1563, AUC: 0.9404, AP: 0.9331\n",
      "Epoch: 161, LOSS: 2.2540, AUC: 0.9402, AP: 0.9329\n",
      "Epoch: 162, LOSS: 2.0818, AUC: 0.9403, AP: 0.9328\n",
      "Epoch: 163, LOSS: 2.1321, AUC: 0.9401, AP: 0.9327\n",
      "Epoch: 164, LOSS: 2.1797, AUC: 0.9403, AP: 0.9333\n",
      "Epoch: 165, LOSS: 2.0744, AUC: 0.9404, AP: 0.9333\n",
      "Epoch: 166, LOSS: 2.0044, AUC: 0.9404, AP: 0.9334\n",
      "Epoch: 167, LOSS: 2.0894, AUC: 0.9407, AP: 0.9337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 168, LOSS: 2.1113, AUC: 0.9409, AP: 0.9338\n",
      "Epoch: 169, LOSS: 2.0991, AUC: 0.9410, AP: 0.9340\n",
      "Epoch: 170, LOSS: 2.0235, AUC: 0.9415, AP: 0.9346\n",
      "Epoch: 171, LOSS: 2.0662, AUC: 0.9417, AP: 0.9352\n",
      "Epoch: 172, LOSS: 1.9541, AUC: 0.9418, AP: 0.9348\n",
      "Epoch: 173, LOSS: 2.0060, AUC: 0.9424, AP: 0.9356\n",
      "Epoch: 174, LOSS: 1.9404, AUC: 0.9425, AP: 0.9358\n",
      "Epoch: 175, LOSS: 2.0180, AUC: 0.9427, AP: 0.9359\n",
      "Epoch: 176, LOSS: 1.9667, AUC: 0.9433, AP: 0.9363\n",
      "Epoch: 177, LOSS: 2.0038, AUC: 0.9432, AP: 0.9361\n",
      "Epoch: 178, LOSS: 2.0398, AUC: 0.9434, AP: 0.9361\n",
      "Epoch: 179, LOSS: 1.9157, AUC: 0.9433, AP: 0.9358\n",
      "Epoch: 180, LOSS: 1.9576, AUC: 0.9435, AP: 0.9358\n",
      "Epoch: 181, LOSS: 2.0142, AUC: 0.9435, AP: 0.9363\n",
      "Epoch: 182, LOSS: 1.9515, AUC: 0.9439, AP: 0.9365\n",
      "Epoch: 183, LOSS: 1.9670, AUC: 0.9441, AP: 0.9364\n",
      "Epoch: 184, LOSS: 1.9359, AUC: 0.9441, AP: 0.9365\n",
      "Epoch: 185, LOSS: 1.9098, AUC: 0.9445, AP: 0.9369\n",
      "Epoch: 186, LOSS: 1.9872, AUC: 0.9444, AP: 0.9368\n",
      "Epoch: 187, LOSS: 1.8830, AUC: 0.9442, AP: 0.9368\n",
      "Epoch: 188, LOSS: 1.8727, AUC: 0.9440, AP: 0.9367\n",
      "Epoch: 189, LOSS: 1.9330, AUC: 0.9438, AP: 0.9368\n",
      "Epoch: 190, LOSS: 1.8426, AUC: 0.9437, AP: 0.9368\n",
      "Epoch: 191, LOSS: 1.8850, AUC: 0.9433, AP: 0.9366\n",
      "Epoch: 192, LOSS: 1.8454, AUC: 0.9432, AP: 0.9365\n",
      "Epoch: 193, LOSS: 1.8540, AUC: 0.9433, AP: 0.9370\n",
      "Epoch: 194, LOSS: 1.8453, AUC: 0.9434, AP: 0.9367\n",
      "Epoch: 195, LOSS: 1.8445, AUC: 0.9437, AP: 0.9371\n",
      "Epoch: 196, LOSS: 1.7328, AUC: 0.9439, AP: 0.9372\n",
      "Epoch: 197, LOSS: 1.8298, AUC: 0.9441, AP: 0.9374\n",
      "Epoch: 198, LOSS: 1.8102, AUC: 0.9445, AP: 0.9379\n",
      "Epoch: 199, LOSS: 1.8056, AUC: 0.9445, AP: 0.9376\n",
      "Epoch: 200, LOSS: 1.8343, AUC: 0.9445, AP: 0.9376\n",
      "Epoch: 201, LOSS: 1.7991, AUC: 0.9447, AP: 0.9378\n",
      "Epoch: 202, LOSS: 1.8135, AUC: 0.9447, AP: 0.9383\n",
      "Epoch: 203, LOSS: 1.7993, AUC: 0.9446, AP: 0.9382\n",
      "Epoch: 204, LOSS: 1.7862, AUC: 0.9445, AP: 0.9379\n",
      "Epoch: 205, LOSS: 1.7059, AUC: 0.9444, AP: 0.9376\n",
      "Epoch: 206, LOSS: 1.8079, AUC: 0.9443, AP: 0.9376\n",
      "Epoch: 207, LOSS: 1.7708, AUC: 0.9441, AP: 0.9372\n",
      "Epoch: 208, LOSS: 1.7447, AUC: 0.9442, AP: 0.9377\n",
      "Epoch: 209, LOSS: 1.8054, AUC: 0.9441, AP: 0.9377\n",
      "Epoch: 210, LOSS: 1.7396, AUC: 0.9444, AP: 0.9380\n",
      "Epoch: 211, LOSS: 1.6973, AUC: 0.9447, AP: 0.9383\n",
      "Epoch: 212, LOSS: 1.7327, AUC: 0.9453, AP: 0.9390\n",
      "Epoch: 213, LOSS: 1.6660, AUC: 0.9458, AP: 0.9392\n",
      "Epoch: 214, LOSS: 1.7495, AUC: 0.9458, AP: 0.9393\n",
      "Epoch: 215, LOSS: 1.7233, AUC: 0.9458, AP: 0.9393\n",
      "Epoch: 216, LOSS: 1.7013, AUC: 0.9457, AP: 0.9393\n",
      "Epoch: 217, LOSS: 1.6434, AUC: 0.9457, AP: 0.9392\n",
      "Epoch: 218, LOSS: 1.6911, AUC: 0.9457, AP: 0.9393\n",
      "Epoch: 219, LOSS: 1.7435, AUC: 0.9456, AP: 0.9393\n",
      "Epoch: 220, LOSS: 1.7175, AUC: 0.9456, AP: 0.9392\n",
      "Epoch: 221, LOSS: 1.6604, AUC: 0.9457, AP: 0.9396\n",
      "Epoch: 222, LOSS: 1.6461, AUC: 0.9461, AP: 0.9397\n",
      "Epoch: 223, LOSS: 1.6916, AUC: 0.9460, AP: 0.9396\n",
      "Epoch: 224, LOSS: 1.6647, AUC: 0.9460, AP: 0.9396\n",
      "Epoch: 225, LOSS: 1.6662, AUC: 0.9460, AP: 0.9395\n",
      "Epoch: 226, LOSS: 1.6730, AUC: 0.9459, AP: 0.9395\n",
      "Epoch: 227, LOSS: 1.6033, AUC: 0.9462, AP: 0.9398\n",
      "Epoch: 228, LOSS: 1.6431, AUC: 0.9464, AP: 0.9398\n",
      "Epoch: 229, LOSS: 1.6153, AUC: 0.9465, AP: 0.9399\n",
      "Epoch: 230, LOSS: 1.6441, AUC: 0.9462, AP: 0.9394\n",
      "Epoch: 231, LOSS: 1.5644, AUC: 0.9462, AP: 0.9396\n",
      "Epoch: 232, LOSS: 1.5891, AUC: 0.9460, AP: 0.9395\n",
      "Epoch: 233, LOSS: 1.6326, AUC: 0.9460, AP: 0.9400\n",
      "Epoch: 234, LOSS: 1.5956, AUC: 0.9462, AP: 0.9401\n",
      "Epoch: 235, LOSS: 1.5728, AUC: 0.9464, AP: 0.9402\n",
      "Epoch: 236, LOSS: 1.6089, AUC: 0.9462, AP: 0.9399\n",
      "Epoch: 237, LOSS: 1.5697, AUC: 0.9461, AP: 0.9400\n",
      "Epoch: 238, LOSS: 1.6019, AUC: 0.9460, AP: 0.9400\n",
      "Epoch: 239, LOSS: 1.5362, AUC: 0.9458, AP: 0.9398\n",
      "Epoch: 240, LOSS: 1.5637, AUC: 0.9459, AP: 0.9396\n",
      "Epoch: 241, LOSS: 1.5494, AUC: 0.9461, AP: 0.9398\n",
      "Epoch: 242, LOSS: 1.5375, AUC: 0.9461, AP: 0.9398\n",
      "Epoch: 243, LOSS: 1.5639, AUC: 0.9464, AP: 0.9400\n",
      "Epoch: 244, LOSS: 1.5730, AUC: 0.9465, AP: 0.9402\n",
      "Epoch: 245, LOSS: 1.5564, AUC: 0.9467, AP: 0.9404\n",
      "Epoch: 246, LOSS: 1.5528, AUC: 0.9465, AP: 0.9403\n",
      "Epoch: 247, LOSS: 1.5240, AUC: 0.9465, AP: 0.9405\n",
      "Epoch: 248, LOSS: 1.4863, AUC: 0.9464, AP: 0.9405\n",
      "Epoch: 249, LOSS: 1.5082, AUC: 0.9463, AP: 0.9405\n",
      "Epoch: 250, LOSS: 1.5063, AUC: 0.9462, AP: 0.9405\n",
      "Epoch: 251, LOSS: 1.5494, AUC: 0.9463, AP: 0.9405\n",
      "Epoch: 252, LOSS: 1.5574, AUC: 0.9466, AP: 0.9406\n",
      "Epoch: 253, LOSS: 1.5457, AUC: 0.9468, AP: 0.9408\n",
      "Epoch: 254, LOSS: 1.5089, AUC: 0.9469, AP: 0.9408\n",
      "Epoch: 255, LOSS: 1.5209, AUC: 0.9470, AP: 0.9408\n",
      "Epoch: 256, LOSS: 1.5331, AUC: 0.9470, AP: 0.9408\n",
      "Epoch: 257, LOSS: 1.4541, AUC: 0.9472, AP: 0.9413\n",
      "Epoch: 258, LOSS: 1.5006, AUC: 0.9472, AP: 0.9415\n",
      "Epoch: 259, LOSS: 1.4694, AUC: 0.9473, AP: 0.9416\n",
      "Epoch: 260, LOSS: 1.5065, AUC: 0.9475, AP: 0.9418\n",
      "Epoch: 261, LOSS: 1.4700, AUC: 0.9474, AP: 0.9417\n",
      "Epoch: 262, LOSS: 1.4988, AUC: 0.9474, AP: 0.9417\n",
      "Epoch: 263, LOSS: 1.4715, AUC: 0.9473, AP: 0.9416\n",
      "Epoch: 264, LOSS: 1.4756, AUC: 0.9477, AP: 0.9418\n",
      "Epoch: 265, LOSS: 1.4591, AUC: 0.9477, AP: 0.9418\n",
      "Epoch: 266, LOSS: 1.4765, AUC: 0.9477, AP: 0.9417\n",
      "Epoch: 267, LOSS: 1.4571, AUC: 0.9478, AP: 0.9417\n",
      "Epoch: 268, LOSS: 1.4527, AUC: 0.9477, AP: 0.9416\n",
      "Epoch: 269, LOSS: 1.4772, AUC: 0.9479, AP: 0.9418\n",
      "Epoch: 270, LOSS: 1.4635, AUC: 0.9482, AP: 0.9421\n",
      "Epoch: 271, LOSS: 1.4799, AUC: 0.9481, AP: 0.9422\n",
      "Epoch: 272, LOSS: 1.4354, AUC: 0.9480, AP: 0.9424\n",
      "Epoch: 273, LOSS: 1.4500, AUC: 0.9476, AP: 0.9422\n",
      "Epoch: 274, LOSS: 1.4176, AUC: 0.9477, AP: 0.9420\n",
      "Epoch: 275, LOSS: 1.4584, AUC: 0.9478, AP: 0.9422\n",
      "Epoch: 276, LOSS: 1.4892, AUC: 0.9480, AP: 0.9424\n",
      "Epoch: 277, LOSS: 1.4403, AUC: 0.9477, AP: 0.9421\n",
      "Epoch: 278, LOSS: 1.3894, AUC: 0.9479, AP: 0.9422\n",
      "Epoch: 279, LOSS: 1.4471, AUC: 0.9475, AP: 0.9423\n",
      "Epoch: 280, LOSS: 1.3688, AUC: 0.9476, AP: 0.9424\n",
      "Epoch: 281, LOSS: 1.4025, AUC: 0.9476, AP: 0.9423\n",
      "Epoch: 282, LOSS: 1.4089, AUC: 0.9476, AP: 0.9422\n",
      "Epoch: 283, LOSS: 1.4238, AUC: 0.9474, AP: 0.9415\n",
      "Epoch: 284, LOSS: 1.3642, AUC: 0.9474, AP: 0.9415\n",
      "Epoch: 285, LOSS: 1.4240, AUC: 0.9475, AP: 0.9419\n",
      "Epoch: 286, LOSS: 1.4310, AUC: 0.9476, AP: 0.9421\n",
      "Epoch: 287, LOSS: 1.3677, AUC: 0.9479, AP: 0.9423\n",
      "Epoch: 288, LOSS: 1.4101, AUC: 0.9481, AP: 0.9427\n",
      "Epoch: 289, LOSS: 1.3910, AUC: 0.9486, AP: 0.9432\n",
      "Epoch: 290, LOSS: 1.3546, AUC: 0.9487, AP: 0.9435\n",
      "Epoch: 291, LOSS: 1.3811, AUC: 0.9486, AP: 0.9436\n",
      "Epoch: 292, LOSS: 1.3716, AUC: 0.9484, AP: 0.9435\n",
      "Epoch: 293, LOSS: 1.3875, AUC: 0.9481, AP: 0.9434\n",
      "Epoch: 294, LOSS: 1.3786, AUC: 0.9479, AP: 0.9433\n",
      "Epoch: 295, LOSS: 1.3676, AUC: 0.9475, AP: 0.9428\n",
      "Epoch: 296, LOSS: 1.4016, AUC: 0.9470, AP: 0.9424\n",
      "Epoch: 297, LOSS: 1.3776, AUC: 0.9470, AP: 0.9423\n",
      "Epoch: 298, LOSS: 1.3756, AUC: 0.9467, AP: 0.9419\n",
      "Epoch: 299, LOSS: 1.3757, AUC: 0.9467, AP: 0.9418\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "for epoch in range(1,epochs):\n",
    "    loss = train()\n",
    "    loss = float(loss)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #valid_pos, valid_neg = data.val_pos_edge_index, data.val_neg_edge_index\n",
    "        auc, ap = test(data.val_pos_edge_index, data.val_neg_edge_index)\n",
    "        print('Epoch: {:03d}, LOSS: {:.4f}, AUC: {:.4f}, AP: {:.4f}'.format(epoch, loss, auc, ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "0d8225a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E3064</td>\n",
       "      <td>0.972802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E298</td>\n",
       "      <td>0.950506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3512</td>\n",
       "      <td>0.907563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E5670</td>\n",
       "      <td>0.999606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E5005</td>\n",
       "      <td>0.765722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>E9179</td>\n",
       "      <td>0.779397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>E5003</td>\n",
       "      <td>0.954958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>E5081</td>\n",
       "      <td>0.812506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>E4705</td>\n",
       "      <td>0.946395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1885</th>\n",
       "      <td>E1012</td>\n",
       "      <td>0.812305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1886 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id      prob\n",
       "0     E3064  0.972802\n",
       "1      E298  0.950506\n",
       "2     E3512  0.907563\n",
       "3     E5670  0.999606\n",
       "4     E5005  0.765722\n",
       "...     ...       ...\n",
       "1881  E9179  0.779397\n",
       "1882  E5003  0.954958\n",
       "1883  E5081  0.812506\n",
       "1884  E4705  0.946395\n",
       "1885  E1012  0.812305\n",
       "\n",
       "[1886 rows x 2 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = model.encode(x, data.test)\n",
    "Upload['prob'] = model.decoder(z,data.test,sigmoid = True).detach().cpu().numpy()\n",
    "Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "224d06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upload.to_csv('output/VGAE_new_'+str(dss_num+1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "577f3b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9466628363497079, 0.9417587022445446)"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(data.val_pos_edge_index, data.val_neg_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d1f82d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
