{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2df931a",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9bdc5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_undirected, from_scipy_sparse_matrix,dense_to_sparse,is_undirected\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import softmax\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Parameter,Embedding\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean, scatter, scatter_add, scatter_max\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b27941b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E370</td>\n",
       "      <td>26</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E667</td>\n",
       "      <td>196</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3190</td>\n",
       "      <td>739</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E848</td>\n",
       "      <td>576</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E2161</td>\n",
       "      <td>466</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   to  from\n",
       "0   E370   26   317\n",
       "1   E667  196   323\n",
       "2  E3190  739   468\n",
       "3   E848  576   156\n",
       "4  E2161  466   199"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"hw2_data/\"\n",
    "dss = ['dataset1','dataset2','dataset3'] #datasets\n",
    "dataset = dict()\n",
    "for ds in dss:\n",
    "    dataset[ds] = dict()\n",
    "    dataset[ds]['content'] = pd.read_csv(path+ds+\"/content.csv\",delimiter = '\\t',header = None)\n",
    "    dataset[ds]['train'] = pd.read_csv(path+ds+\"/train.csv\",delimiter = ',')\n",
    "    dataset[ds]['test'] = pd.read_csv(path+ds+\"/test.csv\",delimiter = ',')\n",
    "    dataset[ds]['upload'] = pd.read_csv(path+ds+\"/upload.csv\",delimiter = ',')\n",
    "dataset[dss[2]]['test'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0da49",
   "metadata": {},
   "source": [
    "## Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "222d51e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 1434)\n",
      "2708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1433,)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[dss[0]]['content'].shape)\n",
    "print(len(dataset[dss[0]]['content']))\n",
    "np.array(dataset[dss[0]]['content'].iloc[0,1:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2563c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8686, 4)\n",
      "8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2397"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[dss[0]]['train'].shape)\n",
    "print(len(dataset[dss[0]]['train']))\n",
    "dataset[dss[0]]['train'].loc[1,'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c400784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edge(0,1,label=0)\n",
    "G.add_edge(0,2,label=1)\n",
    "G.add_edge(1,2,label=1)\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "54b7b149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2, 1), (1, 2, 1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in list(G.edges.data('label')) if e[2]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c6401abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example:\n",
    "dss_num = 0\n",
    "Content = dataset[dss[dss_num]]['content']\n",
    "Train = dataset[dss[dss_num]]['train']\n",
    "Test = dataset[dss[dss_num]]['test']\n",
    "Upload = dataset[dss[dss_num]]['upload']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b5e9f",
   "metadata": {},
   "source": [
    "---\n",
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3980e325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "da085f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd7d9e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b9327be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def floor(x):\n",
    "    return torch.div(x, 1, rounding_mode='trunc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44349bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    # Row-normalize feature matrix and convert to tuple representation\n",
    "    rowsum = np.array(features.sum(1),dtype = np.float32)\n",
    "    rowsum = (rowsum==0)*1+rowsum\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1d2e8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content,train, test):\n",
    "    G = nx.DiGraph()\n",
    "    # for easier split the edges, create 2 graph, 1 with positive edge, the other with given negative edges\n",
    "    G_pos = nx.DiGraph()\n",
    "    G_neg = nx.DiGraph()\n",
    "    graph_node_features_dict = dict()\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        #graph_node_features_dict[content.iloc[i,0]] = np.array(content.iloc[i,1:])\n",
    "        G.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        # pos and neg\n",
    "        G_pos.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        G_neg.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        \n",
    "    for i in range(len(train)):\n",
    "        # Adding nodes into G\n",
    "        '''\n",
    "        if train.loc[i,'from'] not in G:\n",
    "            G.add_node(train.loc[i,'from'],features = graph_node_features_dict[train.loc[i,'from']])\n",
    "        if train.loc[i,'to'] not in G:\n",
    "            G.add_node(train.loc[i,'to'],features = graph_node_features_dict[train.loc[i,'to']])\n",
    "        ''' \n",
    "        # Adding edges\n",
    "        G.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "        \n",
    "        # pos and neg\n",
    "        if train.loc[i,'label'] == 0: \n",
    "            G_neg.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "        else:\n",
    "            G_pos.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "\n",
    "    adj = nx.adjacency_matrix(G,sorted(G.nodes()))\n",
    "    adj_pos = nx.adjacency_matrix(G_pos,sorted(G_pos.nodes()))\n",
    "    adj_neg = nx.adjacency_matrix(G_neg,sorted(G_neg.nodes()))\n",
    "    features = np.array(\n",
    "        [features for _, features in sorted(G.nodes(data='features'), key=lambda x: x[0])])\n",
    "    features = preprocess_features(features)\n",
    "    # Skip train,valid,and test mask\n",
    "    \n",
    "    num_features = features.shape[1]\n",
    "    features = torch.FloatTensor(features)\n",
    "    return adj, adj_pos, adj_neg, features, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "56e6b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, pos, neg, features, num_features = load_data(Content,Train,Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6a433235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges(data):\n",
    "    set_random_seed(42) #Seed is randomly set\n",
    "    \n",
    "    # ? This select the edges that from<to, but why? Don't understand, not using this\n",
    "    row, col = data.edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "    \n",
    "    val_ratio = 0.1\n",
    "    #test_ratio = 0.1\n",
    "    \n",
    "    #split positive edges\n",
    "    row_pos, col_pos = data.pos\n",
    "    mask_pos = row_pos < col_pos\n",
    "    row_pos, col_pos = row_pos[mask_pos], col_pos[mask_pos]\n",
    "\n",
    "    n_v = floor(val_ratio * row_pos.size(0)).int() #number of validation positive edges\n",
    "    #n_t = floor(test_ratio * row_pos.size(0)).int() #number of test positive edges\n",
    "    \n",
    "    perm = torch.randperm(row_pos.size(0))\n",
    "    row_pos, col_pos = row_pos[perm], col_pos[perm]\n",
    "    r, c = row_pos[:n_v], col_pos[:n_v]\n",
    "    data.val_pos = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_pos[n_v:n_v+n_t], col_pos[n_v:n_v+n_t]\n",
    "    #data.test_pos = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_pos[n_v+n_t:], col_pos[n_v+n_t:]\n",
    "    r, c = row_pos[n_v:], col_pos[n_v:]\n",
    "    data.train_pos = torch.stack([r, c], dim=0)\n",
    "    print(\"Positive edges shape(train,val):\",end=\"\")\n",
    "    #print(data.train_pos.shape,data.test_pos.shape,data.val_pos.shape)\n",
    "    print(data.train_pos.shape,data.val_pos.shape)\n",
    "    \n",
    "    #split neg edges\n",
    "    row_neg, col_neg = data.neg\n",
    "    mask_neg = row_neg < col_neg\n",
    "    row_neg, col_neg = row_neg[mask_neg], col_neg[mask_neg]\n",
    "    \n",
    "    '''\n",
    "    n_v = floor(val_ratio * row_neg.size(0)).int() #number of validation positive edges\n",
    "    #n_t = floor(test_ratio * row_neg.size(0)).int() #number of test positive edges\n",
    "    \n",
    "    perm = torch.randperm(row_neg.size(0))\n",
    "    row_neg, col_neg = row_neg[perm], col_neg[perm]\n",
    "    r, c = row_neg[:n_v], col_neg[:n_v]\n",
    "    data.val_neg = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_neg[n_v:n_v+n_t], col_neg[n_v:n_v+n_t]\n",
    "    #data.test_neg = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_neg[n_v+n_t:], col_neg[n_v+n_t:]\n",
    "    r, c = row_neg[n_v:], col_neg[n_v:]\n",
    "    data.train_neg = torch.stack([r, c], dim=0)\n",
    "    print(\"Negative edges shape(train,val):\",end=\"\")\n",
    "    #print(data.train_neg.shape,data.test_neg.shape,data.val_neg.shape)\n",
    "    print(data.train_neg.shape,data.val_neg.shape)\n",
    "    '''\n",
    "    n_v = floor(val_ratio * row_neg.size(0)).int() #number of validation positive edges\n",
    "    neg_adj_mask = torch.ones(data.num_nodes, data.num_nodes, dtype=torch.uint8)\n",
    "    neg_adj_mask = neg_adj_mask.triu(diagonal=1).to(torch.bool)\n",
    "    neg_adj_mask[row, col] = 0\n",
    "\n",
    "    # Sample all the negative edges and split into val, test, train negs\n",
    "    neg_row, neg_col = neg_adj_mask.nonzero(as_tuple=False).t()\n",
    "    perm = torch.randperm(neg_row.size(0))[:row.size(0)]\n",
    "    neg_row, neg_col = neg_row[perm], neg_col[perm]\n",
    "\n",
    "    row, col = neg_row[:n_v], neg_col[:n_v]\n",
    "    data.val_neg = torch.stack([row, col], dim=0)\n",
    "\n",
    "    row, col = neg_row[n_v:], neg_col[n_v:]\n",
    "    data.train_neg = torch.stack([row, col], dim=0)\n",
    "\n",
    "    \n",
    "    #costruct real test edges\n",
    "    row_test,col_test = data.test\n",
    "    data.test = torch.stack([row_test,col_test],dim = 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bebf0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_init_attribute_representation(data):\n",
    "    # Construct data_observed and compute its node attributes & representation\n",
    "    # Why create undirected edge? Don't use this\n",
    "    # Now pretend args.observe_val_and_injection = False\n",
    "    #print('Here')\n",
    "    #print(data.train_pos.shape,data.train_pos[[1,0],:].shape)\n",
    "    #print(data.train_pos,data.train_pos[[1,0],:])\n",
    "    #print(data.train_pos[:,:],,data.train_pos[[1,0],:])\n",
    "    edge_index = torch.cat((data.train_pos,data.train_pos[[1,0],:]),dim=1)\n",
    "    #print(edge_index)\n",
    "    #edge_index = data.train_pos\n",
    "    #edge_index=torch.cat((edge_index,data.val_pos,data.val_pos[[1,0],:]),dim=1)\n",
    "    data_observed = Data(edge_index=edge_index)\n",
    "    data_observed.num_nodes = data.num_nodes\n",
    "    #use the injection trick and add val data in observed graph \n",
    "    #edge_index_observed = torch.cat((data_observed.edge_index,\\\n",
    "    #    data.train_neg,data.train_neg[[1,0],:],data.val_neg,data.val_neg[[1,0],:]),dim=1)\n",
    "    edge_index_observed = data_observed.edge_index\n",
    "  \n",
    "    #generate the initial node attribute if there isn't any\n",
    "    init_attribute = 'n2v' # Can choose between 'n2v','one_hot','spc','ones','zeros',\n",
    "    # But we already prepare features so it doesn't matter\n",
    "    if data.x == None:\n",
    "        if init_attribute =='n2v':\n",
    "            from node2vec import CalN2V\n",
    "            x = CalN2V(edge_index_observed)#,args)\n",
    "        if init_attribute =='one_hot':\n",
    "            x = F.one_hot(torch.arange(data.num_nodes), num_classes=data.num_nodes)\n",
    "            x = x.float()\n",
    "        if init_attribute =='spc':\n",
    "            from SPC import spc\n",
    "            x = spc(edge_index_observed)#,args)\n",
    "            x = x.float()\n",
    "        if init_attribute =='ones':\n",
    "            embedding_dim = 32\n",
    "            x = torch.ones(data.num_nodes,args.embedding_dim)\n",
    "            x = x.float()\n",
    "        if init_attribute =='zeros':\n",
    "            embedding_dim = 32\n",
    "            x = torch.zeros(data.num_nodes,args.embedding_dim)\n",
    "            x = x.float()\n",
    "    else:\n",
    "        x = data.x\n",
    "    #generate the initial node representation using unsupervised models\n",
    "    \n",
    "    init_representation = None # Don't understand what this part doing\n",
    "    if init_representation != None:\n",
    "        val_and_test=[data.test_pos,data.test_neg,data.val_pos,data.val_neg]\n",
    "        num_nodes,_=x.shape\n",
    "        #add self-loop for the last node to aviod losing node if the last node dosen't have a link.\n",
    "        if (num_nodes-1) in edge_index_observed:\n",
    "            edge_index_observed=edge_index_observed.clone().detach()\n",
    "        else:\n",
    "            edge_index_observed=torch.cat((edge_index_observed.clone().detach(),torch.tensor([[num_nodes-1],[num_nodes-1]])),dim=1)\n",
    "        if init_representation == 'gic':\n",
    "            par_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\"))\n",
    "            sys.path.append('%s/software/GIC/' % args.par_dir)\n",
    "            from GICEmbs import CalGIC\n",
    "            data_observed.x, auc, ap = CalGIC(edge_index_observed, x, args.data_name, val_and_test,args)\n",
    "\n",
    "        if init_representation == 'vgae':\n",
    "            from vgae import CalVGAE\n",
    "            data_observed.x, auc, ap = CalVGAE(edge_index_observed, x, val_and_test, args)\n",
    "        if init_representation == 'svgae':\n",
    "            from svgae import CalSVGAE\n",
    "            data_observed.x, auc, ap = CalSVGAE(edge_index_observed, x, val_and_test, args)\n",
    "        if init_representation == 'argva':\n",
    "            from argva import CalARGVA\n",
    "            data_observed.x, auc, ap = CalARGVA(edge_index_observed, x, val_and_test, args)\n",
    "        feature_results=[auc,ap]\n",
    "    else:\n",
    "        data_observed.x = x\n",
    "        feature_results=None\n",
    "\n",
    "    return data_observed,feature_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ce585441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_hop_subgraph(node_idx, num_hops, edge_index, max_nodes_per_hop = None,num_nodes = None):\n",
    "  \n",
    "    if num_nodes == None:\n",
    "        num_nodes = torch.max(edge_index)+1\n",
    "    row, col = edge_index\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    node_idx = node_idx.to(row.device)\n",
    "\n",
    "    subsets = [node_idx]\n",
    "\n",
    "    if max_nodes_per_hop == None:\n",
    "        for _ in range(num_hops):\n",
    "            node_mask.fill_(False)\n",
    "            node_mask[subsets[-1]] = True\n",
    "            torch.index_select(node_mask, 0, row, out = edge_mask)\n",
    "            subsets.append(col[edge_mask])\n",
    "    else:\n",
    "        not_visited = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "        not_visited.fill_(True)\n",
    "        for _ in range(num_hops):\n",
    "            node_mask.fill_(False)# the source node mask in this hop\n",
    "            node_mask[subsets[-1]] = True #mark the sources\n",
    "            not_visited[subsets[-1]] = False # mark visited nodes\n",
    "            torch.index_select(node_mask, 0, row, out = edge_mask) # indices of all neighbors\n",
    "            neighbors = col[edge_mask].unique() #remove repeats\n",
    "            neighbor_mask = row.new_empty(num_nodes, dtype=torch.bool) # mask of all neighbor nodes\n",
    "            edge_mask_hop = row.new_empty(row.size(0), dtype=torch.bool) # selected neighbor mask in this hop\n",
    "            neighbor_mask.fill_(False)\n",
    "            neighbor_mask[neighbors] = True\n",
    "            neighbor_mask = torch.logical_and(neighbor_mask, not_visited) # all neighbors that are not visited\n",
    "            ind = torch.where(neighbor_mask==True) #indicies of all the unvisited neighbors\n",
    "            if ind[0].size(0) > max_nodes_per_hop:\n",
    "                perm = torch.randperm(ind[0].size(0))\n",
    "                ind = ind[0][perm]\n",
    "                neighbor_mask[ind[max_nodes_per_hop:]] = False # randomly select max_nodes_per_hop nodes\n",
    "                torch.index_select(neighbor_mask, 0, col, out = edge_mask_hop)# find the indicies of selected nodes\n",
    "                edge_mask = torch.logical_and(edge_mask,edge_mask_hop) # change edge_mask\n",
    "            subsets.append(col[edge_mask])\n",
    "\n",
    "    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
    "    inv = inv[:node_idx.numel()]\n",
    "\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "    edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    node_idx = row.new_full((num_nodes, ), -1)\n",
    "    node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "    edge_index = node_idx[edge_index]\n",
    "\n",
    "    return subset, edge_index, inv, edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ac382c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_edge(data_observed, label, p_edge):\n",
    "    num_hops = 2 # number of hops in sampling subgraph, set 2 which same as default\n",
    "    max_nodes_per_hop =  None #when the graph is too large or too dense, \n",
    "    #we need max node per hop threshold to avoid OOM. Default is None\n",
    "    nodes, edge_index_m, mapping, _ = k_hop_subgraph(node_idx= p_edge, num_hops= num_hops,\\\n",
    " edge_index = data_observed.edge_index, max_nodes_per_hop= max_nodes_per_hop ,num_nodes=data_observed.num_nodes)\n",
    "    x_sub = data_observed.x[nodes,:]\n",
    "    edge_index_p = edge_index_m\n",
    "    edge_index_p = torch.cat((edge_index_p, mapping.view(-1,1)),dim=1)\n",
    "    edge_index_p = torch.cat((edge_index_p, mapping[[1,0]].view(-1,1)),dim=1)\n",
    "\n",
    "    #edge_mask marks the edge under perturbation, i.e., the candidate edge for LP\n",
    "    edge_mask = torch.ones(edge_index_p.size(1),dtype=torch.bool)\n",
    "    edge_mask[-1] = False\n",
    "    edge_mask[-2] = False\n",
    "    \n",
    "    '''\n",
    "    # Run here for drnl labeling\n",
    "    if drnl == True:\n",
    "        num_nodes = torch.max(edge_index_p)+1\n",
    "        z = drnl_node_labeling(edge_index_m, mapping[0],mapping[1],num_nodes)\n",
    "        data = Data(edge_index = edge_index_p, x = x_sub, z = z)\n",
    "    '''\n",
    "    data = Data(edge_index = edge_index_p, x = x_sub, z = 0)\n",
    "    data.edge_mask = edge_mask\n",
    "\n",
    "    #label = 1 if the candidate link (p_edge) is positive and label=0 otherwise\n",
    "    data.label = float(label)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "59e2764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_edge(data_observed, label, p_edge):\n",
    "    num_hops = 2 # number of hops in sampling subgraph, set 2 which same as default\n",
    "    max_nodes_per_hop =  None #when the graph is too large or too dense, \n",
    "    #we need max node per hop threshold to avoid OOM. Default is None\n",
    "    \n",
    "    nodes, edge_index_p, mapping,_ = k_hop_subgraph(node_idx= p_edge, num_hops=num_hops,\\\n",
    " edge_index = data_observed.edge_index,max_nodes_per_hop=max_nodes_per_hop, num_nodes = data_observed.num_nodes)\n",
    "    x_sub = data_observed.x[nodes,:]\n",
    "\n",
    "    #edge_mask marks the edge under perturbation, i.e., the candidate edge for LP\n",
    "    edge_mask = torch.ones(edge_index_p.size(1), dtype = torch.bool)\n",
    "    ind = torch.where((edge_index_p == mapping.view(-1,1)).all(dim=0))\n",
    "    edge_mask[ind[0]] = False\n",
    "    ind = torch.where((edge_index_p == mapping[[1,0]].view(-1,1)).all(dim=0))\n",
    "    edge_mask[ind[0]] = False\n",
    "    '''\n",
    "    if args.drnl == True:\n",
    "        num_nodes = torch.max(edge_index_p)+1\n",
    "        z = drnl_node_labeling(edge_index_p[:,edge_mask], mapping[0],mapping[1],num_nodes)\n",
    "        data = Data(edge_index = edge_index_p, x= x_sub,z = z)\n",
    "    else:\n",
    "    '''\n",
    "    data = Data(edge_index = edge_index_p, x= x_sub,z = 0)\n",
    "    data.edge_mask = edge_mask\n",
    "\n",
    "    #label = 1 if the candidate link (p_edge) is positive and label=0 otherwise\n",
    "    data.label = float(label)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cbe9c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "def prepare_data(content,train,test):\n",
    "    g, pos, neg, features, num_features = load_data(content,train,test)\n",
    "    A = g.toarray()\n",
    "    edge_index,_ = dense_to_sparse(torch.tensor(A))\n",
    "    Apos = pos.toarray()\n",
    "    edge_index_pos,_ = dense_to_sparse(torch.tensor(Apos))\n",
    "    Aneg = neg.toarray()\n",
    "    edge_index_neg,_ = dense_to_sparse(torch.tensor(Aneg))\n",
    "    data = Data(edge_index=edge_index,x=features.to(torch.float),\n",
    "                pos = edge_index_pos,neg = edge_index_neg,test = torch.tensor([Test['from'],Test['to']]))\n",
    "    data = split_edges(data)\n",
    "    data.edge_index = to_undirected(data.edge_index)\n",
    "    \n",
    "\n",
    "    set_random_seed(42) # Random set the seed\n",
    "    data_observed,feature_results= set_init_attribute_representation(data)\n",
    "    # Construct train, val and test data loader\n",
    "    set_random_seed(42)\n",
    "    train_graphs = []\n",
    "    val_graphs = []\n",
    "    test_graphs = []\n",
    "    for i in range(data.train_pos.size(1)):\n",
    "        train_graphs.append(minus_edge(data_observed,1,data.train_pos[:,i]))\n",
    "\n",
    "    for i in range(data.train_neg.size(1)):\n",
    "        train_graphs.append(plus_edge(data_observed,0,data.train_neg[:,i]))\n",
    "    '''\n",
    "    for i in range(data.test_pos.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,1,data.test_pos[:,i]))\n",
    "\n",
    "    for i in range(data.test_neg.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,0,data.test_neg[:,i]))   \n",
    "    '''\n",
    "    for i in range(data.test.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,1,data.test[:,i]))\n",
    "    \n",
    "    #if args.observe_val_and_injection == False:\n",
    "    # pretend args.observe_val_and_injection == False\n",
    "    for i in range(data.val_pos.size(1)):\n",
    "        val_graphs.append(plus_edge(data_observed,1,data.val_pos[:,i]))\n",
    "\n",
    "    for i in range(data.val_neg.size(1)):\n",
    "        val_graphs.append(plus_edge(data_observed,0,data.val_neg[:,i]))\n",
    "    '''\n",
    "    else:\n",
    "        for i in range(data.val_pos.size(1)):\n",
    "            val_graphs.append(minus_edge(data_observed,1,data.val_pos[:,i]))\n",
    "\n",
    "        for i in range(data.val_neg.size(1)):\n",
    "            val_graphs.append(plus_edge(data_observed,0,data.val_neg[:,i]))\n",
    "    '''\n",
    "\n",
    "    \n",
    "    print('Train_link:', str(len(train_graphs)),\n",
    "          ' Val_link:',str(len(val_graphs)),' Test_link:',str(len(test_graphs)))\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_graphs,batch_size=batch_size,shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs,batch_size=batch_size,shuffle=True)\n",
    "    test_loader = DataLoader(test_graphs,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader,feature_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b992a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive edges shape(train,val):torch.Size([2, 1945]) torch.Size([2, 216])\n",
      "Train_link: 5988  Val_link: 425  Test_link: 2172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch_geometric.loader.dataloader.DataLoader at 0x1f787a27688>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader,feature_results = prepare_data(Content,Train,Test)\n",
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c71d7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d59a72d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = next(iter(train_loader)).x.size(1)\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7dd565f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimention of features after concatenation: 1433\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimention of features after concatenation:\",num_features)\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e3351",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aeb948f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e3995fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPred(MessagePassing):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, heads: int = 1,\\\n",
    "                 walk_len: int = 6, drnl: bool = False, z_max: int =100, MSE: bool=True):\n",
    "        super(LinkPred, self).__init__()\n",
    "\n",
    "        self.drnl = drnl\n",
    "        if drnl == True:\n",
    "            self.z_embedding = Embedding(z_max, hidden_channels)\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.wp = WalkPooling(in_channels + hidden_channels*2,\\\n",
    "            hidden_channels, heads, walk_len)\n",
    "\n",
    "        L=walk_len*5+1\n",
    "        self.classifier = MLP(L*heads,MSE=MSE)\n",
    "        self.norm = nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_mask, batch, z = None):\n",
    "        \n",
    "        #using drnl\n",
    "        if self.drnl == True:\n",
    "            z_emb = self.z_embedding(z)\n",
    "            if z_emb.ndim == 3:  # in case z has multiple integer labels\n",
    "                z_emb = z_emb.sum(dim=1)\n",
    "            z_emb = z_emb.view(x.size(0),-1)\n",
    "            x = torch.cat((x,z_emb.view(x.size(0),-1)),dim=1)\n",
    "        \n",
    "        #GCN layers\n",
    "        x_out = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x_out = torch.cat((x_out,x),dim=1)\n",
    "        x = x.relu()\n",
    "        x = self.norm(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.norm(x)\n",
    "        x_out = torch.cat((x_out,x),dim=1)\n",
    "\n",
    "        #Walk Pooling\n",
    "        feature_list = self.wp(x_out, edge_index, edge_mask, batch)\n",
    "\n",
    "        #Classifier\n",
    "        out = self.classifier(feature_list)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class WalkPooling(MessagePassing):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, heads: int = 1,\\\n",
    "                 walk_len: int = 6):\n",
    "        super(WalkPooling, self).__init__()\n",
    "\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.heads = heads\n",
    "        self.walk_len = walk_len\n",
    "\n",
    "        # the linear layers in the attention encoder\n",
    "        self.lin_key1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin_query1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin_key2 = Linear(hidden_channels, heads * hidden_channels)\n",
    "        self.lin_query2 = Linear(hidden_channels, heads * hidden_channels)\n",
    "    def attention_mlp(self, x, edge_index):\n",
    "    \n",
    "        query = self.lin_key1(x).reshape(-1,self.hidden_channels)\n",
    "        key = self.lin_query1(x).reshape(-1,self.hidden_channels)\n",
    "\n",
    "        query = F.leaky_relu(query,0.2)\n",
    "        key = F.leaky_relu(key,0.2)\n",
    "\n",
    "        query = F.dropout(query, p=0.5, training=self.training)\n",
    "        key = F.dropout(key, p=0.5, training=self.training)\n",
    "\n",
    "        query = self.lin_key2(query).view(-1, self.heads, self.hidden_channels)\n",
    "        key = self.lin_query2(key).view(-1, self.heads, self.hidden_channels)\n",
    "\n",
    "        row, col = edge_index\n",
    "        weights = (query[row] * key[col]).sum(dim=-1) / np.sqrt(self.hidden_channels)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def weight_encoder(self, x, edge_index, edge_mask):        \n",
    "     \n",
    "        weights = self.attention_mlp(x, edge_index)\n",
    "    \n",
    "        omega = torch.sigmoid(weights[torch.logical_not(edge_mask)])\n",
    "        \n",
    "        row, col = edge_index\n",
    "        num_nodes = torch.max(edge_index)+1\n",
    "\n",
    "        # edge weights of the plus graph\n",
    "        weights_p = softmax(weights,edge_index[1])\n",
    "\n",
    "        # edge weights of the minus graph\n",
    "        weights_m = weights - scatter_max(weights, col, dim=0, dim_size=num_nodes)[0][col]\n",
    "        weights_m = torch.exp(weights_m)\n",
    "        weights_m = weights_m * edge_mask.view(-1,1)\n",
    "        norm = scatter_add(weights_m, col, dim=0, dim_size=num_nodes)[col] + 1e-16\n",
    "        weights_m = weights_m / norm\n",
    "\n",
    "        return weights_p, weights_m, omega\n",
    "\n",
    "    def forward(self, x, edge_index, edge_mask, batch):\n",
    "        \n",
    "        #encode the node representation into edge weights via attention mechanism\n",
    "        weights_p, weights_m, omega = self.weight_encoder(x, edge_index, edge_mask)\n",
    "\n",
    "        # pytorch geometric set the batch adjacency matrix to\n",
    "        # be the diagonal matrix with each graph's adjacency matrix\n",
    "        # stacked in the diagonal. Therefore, calculating the powers\n",
    "        # of the stochastic matrix directly will cost lots of memory.\n",
    "        # We compute the powers of stochastic matrix as follows\n",
    "        # Let A = diag ([A_1,\\cdots,A_n]) be the batch adjacency matrix,\n",
    "        # we set x = [x_1,\\cdots,x_n]^T be the batch feature matrix\n",
    "        # for the i-th graph in the batch with n_i nodes, its feature \n",
    "        # is a n_i\\times n_max matrix, where n_max is the largest number\n",
    "        # of nodes for all graphs in the batch. The elements of x_i are\n",
    "        # (x_i)_{x,y} = \\delta_{x,y}. \n",
    "\n",
    "        # number of graphs in the batch\n",
    "        batch_size = torch.max(batch)+1\n",
    "\n",
    "        # for node i in the batched graph, index[i] is i's id in the graph before batch \n",
    "        index = torch.zeros(batch.size(0),1,dtype=torch.long)\n",
    "        \n",
    "        # numer of nodes in each graph\n",
    "        _, counts = torch.unique(batch, sorted=True, return_counts=True)\n",
    "        \n",
    "        # maximum number of nodes for all graphs in the batch\n",
    "        max_nodes = torch.max(counts)\n",
    "\n",
    "        # set the values in index\n",
    "        id_start = 0\n",
    "        for i in range(batch_size):\n",
    "            index[id_start:id_start+counts[i]] = torch.arange(0,counts[i],dtype=torch.long).view(-1,1)\n",
    "            id_start = id_start+counts[i]\n",
    "\n",
    "        index = index.to(device)\n",
    "        \n",
    "        #the output graph features of walk pooling\n",
    "        nodelevel_p = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        nodelevel_m = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        linklevel_p = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        linklevel_m = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        graphlevel = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        # a link (i,j) has two directions i->j and j->i, and\n",
    "        # when extract the features of the link, we usually average over\n",
    "        # the two directions. indices_odd and indices_even records the\n",
    "        # indices for a link in two directions\n",
    "        indices_odd = torch.arange(0,omega.size(0),2).to(device)\n",
    "        indices_even = torch.arange(1,omega.size(0),2).to(device)\n",
    "\n",
    "        omega = torch.index_select(omega, 0 ,indices_even)\\\n",
    "        + torch.index_select(omega,0,indices_odd)\n",
    "        \n",
    "        #node id of the candidate (or perturbation) link\n",
    "        link_ij, link_ji = edge_index[:,torch.logical_not(edge_mask)]\n",
    "        node_i = link_ij[indices_odd]\n",
    "        node_j = link_ij[indices_even]\n",
    "\n",
    "        # compute the powers of stochastic matrix\n",
    "        for head in range(self.heads):\n",
    "\n",
    "            # x on the plus graph and minus graph\n",
    "            x_p = torch.zeros(batch.size(0),max_nodes,dtype=x.dtype).to(device)\n",
    "            x_p = x_p.scatter_(1,index,1)\n",
    "            x_m = torch.zeros(batch.size(0),max_nodes,dtype=x.dtype).to(device)\n",
    "            x_m = x_m.scatter_(1,index,1)\n",
    "\n",
    "            # propagage once\n",
    "            x_p = self.propagate(edge_index, x= x_p, norm = weights_p[:,head])\n",
    "            x_m = self.propagate(edge_index, x= x_m, norm = weights_m[:,head])\n",
    "        \n",
    "            # start from tau = 2\n",
    "            for i in range(self.walk_len):\n",
    "                #print(f\"self.walk_len:{i}\")\n",
    "                x_p = self.propagate(edge_index, x= x_p, norm = weights_p[:,head])\n",
    "                x_m = self.propagate(edge_index, x= x_m, norm = weights_m[:,head])\n",
    "                \n",
    "                # returning probabilities around i + j\n",
    "                nodelevel_p_w = x_p[node_i,index[node_i].view(-1)] + x_p[node_j,index[node_j].view(-1)]\n",
    "                nodelevel_m_w = x_m[node_i,index[node_i].view(-1)] + x_m[node_j,index[node_j].view(-1)]\n",
    "                nodelevel_p[:,head*self.walk_len+i] = nodelevel_p_w.view(-1)\n",
    "                nodelevel_m[:,head*self.walk_len+i] = nodelevel_m_w.view(-1)\n",
    "  \n",
    "                # transition probabilities between i and j\n",
    "                linklevel_p_w = x_p[node_i,index[node_j].view(-1)] + x_p[node_j,index[node_i].view(-1)]\n",
    "                linklevel_m_w = x_m[node_i,index[node_j].view(-1)] + x_m[node_j,index[node_i].view(-1)]\n",
    "                linklevel_p[:,head*self.walk_len+i] = linklevel_p_w.view(-1)\n",
    "                linklevel_m[:,head*self.walk_len+i] = linklevel_m_w.view(-1)\n",
    "\n",
    "                # graph average of returning probabilities\n",
    "                diag_ele_p = torch.gather(x_p,1,index)\n",
    "                diag_ele_m = torch.gather(x_m,1,index)\n",
    "\n",
    "                graphlevel_p = scatter_add(diag_ele_p, batch, dim = 0)\n",
    "                graphlevel_m = scatter_add(diag_ele_m, batch, dim = 0)\n",
    "\n",
    "                graphlevel[:,head*self.walk_len+i] = (graphlevel_p-graphlevel_m).view(-1)\n",
    "         \n",
    "        feature_list = graphlevel \n",
    "        feature_list = torch.cat((feature_list,omega),dim=1)\n",
    "        feature_list = torch.cat((feature_list,nodelevel_p),dim=1)\n",
    "        feature_list = torch.cat((feature_list,nodelevel_m),dim=1)\n",
    "        feature_list = torch.cat((feature_list,linklevel_p),dim=1)\n",
    "        feature_list = torch.cat((feature_list,linklevel_m),dim=1)\n",
    "\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j  \n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    # adopt a MLP as classifier for graphs\n",
    "    def __init__(self,input_size,MSE=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.nn = nn.BatchNorm1d(input_size)\n",
    "        self.linear1 = torch.nn.Linear(input_size,input_size*20)\n",
    "        self.linear2 = torch.nn.Linear(input_size*20,input_size*20)\n",
    "        self.linear3 = torch.nn.Linear(input_size*20,input_size*10)\n",
    "        self.linear4 = torch.nn.Linear(input_size*10,input_size)\n",
    "        self.linear5 = torch.nn.Linear(input_size,1)\n",
    "        self.act= nn.ReLU()\n",
    "        self.MSE=MSE\n",
    "        self.norm1 = nn.BatchNorm1d(input_size*20)\n",
    "        self.norm2 = nn.BatchNorm1d(input_size*20)\n",
    "        self.norm3 = nn.BatchNorm1d(input_size*10)\n",
    "        self.norm4 = nn.BatchNorm1d(input_size)\n",
    "    def forward(self, x):\n",
    "        out= self.nn(x)\n",
    "        out= self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm1(out)\n",
    "        out= self.linear2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm4(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        out = self.linear5(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        if self.MSE:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bd176aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader,epoch):\n",
    "    model.train()\n",
    "    loss_epoch=0\n",
    "    for data in tqdm(loader,desc=\"train\"):  # Iterate in batches over the training dataset.\n",
    "        data = data.to(device)\n",
    "        label= data.label\n",
    "        out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "        torch.cuda.empty_cache()\n",
    "        loss = criterion(out.view(-1), label)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        loss_epoch=loss_epoch+loss.item()\n",
    "    return loss_epoch/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9c6c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader,data_type='test'):\n",
    "    model.eval()\n",
    "    scores = torch.tensor([])\n",
    "    labels = torch.tensor([])\n",
    "    loss_total=0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader,desc='test:'+data_type):  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "            loss = criterion(out.view(-1), data.label)\n",
    "            out = out.cpu().clone().detach()\n",
    "            scores = torch.cat((scores,out),dim = 0)\n",
    "            labels = torch.cat((labels,data.label.view(-1,1).cpu().clone().detach()),dim = 0)\n",
    "        scores = scores.cpu().clone().detach().numpy()\n",
    "        labels = labels.cpu().clone().detach().numpy()\n",
    "        loss_total=loss_total+loss.item()\n",
    "        return roc_auc_score(labels, scores), average_precision_score(labels, scores),loss_total,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fe26ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_len = 6\n",
    "lr = 0.005\n",
    "heads = 2\n",
    "hidden_channels = 256\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "z_max = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2f2c86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPred(in_channels = num_features, hidden_channels = hidden_channels,\\\n",
    "    heads = heads, walk_len = walk_len, drnl = False,z_max = z_max, MSE= True).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=0.005)\n",
    "#criterion = torch.nn.MSELoss(reduction='mean')\n",
    "criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "040d522d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.95it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss : 0.6953,     Val Loss : 0.6911, Val AUC: 0.4657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.65it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss : 0.6761,     Val Loss : 0.6604, Val AUC: 0.5216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.89it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss : 0.6712,     Val Loss : 0.7362, Val AUC: 0.6808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.25it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss : 0.6704,     Val Loss : 0.6533, Val AUC: 0.6725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:22<00:00,  8.49it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss : 0.6675,     Val Loss : 0.6956, Val AUC: 0.6786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.91it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 24.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss : 0.6703,     Val Loss : 0.6984, Val AUC: 0.7123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.94it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 24.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss : 0.6698,     Val Loss : 0.4486, Val AUC: 0.6773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.05it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 25.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss : 0.6723,     Val Loss : 0.6941, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  8.97it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 18.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss : 0.6987,     Val Loss : 0.6861, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:22<00:00,  8.45it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss : 0.6987,     Val Loss : 0.6979, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.69it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss : 0.6984,     Val Loss : 0.7050, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.59it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss : 0.6985,     Val Loss : 0.6983, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.70it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss : 0.6987,     Val Loss : 0.6945, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:22<00:00,  8.19it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 19.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss : 0.6984,     Val Loss : 0.6916, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:22<00:00,  8.31it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss : 0.6986,     Val Loss : 0.6917, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.73it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss : 0.6984,     Val Loss : 0.7029, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.74it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss : 0.6985,     Val Loss : 0.6917, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.76it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss : 0.6985,     Val Loss : 0.6945, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.88it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss : 0.6985,     Val Loss : 0.6888, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.74it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss : 0.6984,     Val Loss : 0.6915, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:22<00:00,  8.27it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss : 0.6985,     Val Loss : 0.6980, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.63it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Loss : 0.6985,     Val Loss : 0.6886, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.08it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Loss : 0.6985,     Val Loss : 0.6918, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.03it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Loss : 0.6985,     Val Loss : 0.6883, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.64it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 25.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Loss : 0.6984,     Val Loss : 0.6981, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.81it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss : 0.6986,     Val Loss : 0.6920, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.81it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Loss : 0.6984,     Val Loss : 0.6917, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.70it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Loss : 0.6984,     Val Loss : 0.6981, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.78it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 028, Loss : 0.6986,     Val Loss : 0.6872, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.74it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Loss : 0.6984,     Val Loss : 0.6888, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.91it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 030, Loss : 0.6984,     Val Loss : 0.6945, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.92it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 031, Loss : 0.6985,     Val Loss : 0.6946, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.62it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 19.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 032, Loss : 0.6984,     Val Loss : 0.6914, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.81it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 033, Loss : 0.6985,     Val Loss : 0.6879, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.85it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 034, Loss : 0.6985,     Val Loss : 0.6916, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.59it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 035, Loss : 0.6985,     Val Loss : 0.6828, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:22<00:00,  8.29it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 036, Loss : 0.6985,     Val Loss : 0.6917, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  8.97it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 037, Loss : 0.6985,     Val Loss : 0.6884, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.32it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 038, Loss : 0.6984,     Val Loss : 0.6836, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.06it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 039, Loss : 0.6984,     Val Loss : 0.6916, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.31it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 040, Loss : 0.6986,     Val Loss : 0.6944, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.15it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 041, Loss : 0.6986,     Val Loss : 0.6891, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:19<00:00,  9.48it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 042, Loss : 0.6984,     Val Loss : 0.6978, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.72it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 043, Loss : 0.6984,     Val Loss : 0.6949, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.94it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 044, Loss : 0.6985,     Val Loss : 0.6898, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.78it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 045, Loss : 0.6983,     Val Loss : 0.6890, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:21<00:00,  8.84it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 046, Loss : 0.6985,     Val Loss : 0.6945, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.07it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 047, Loss : 0.6984,     Val Loss : 0.6966, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.36it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 048, Loss : 0.6984,     Val Loss : 0.6915, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 188/188 [00:20<00:00,  9.19it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 049, Loss : 0.6985,     Val Loss : 0.6975, Val AUC: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Best_Val_fromloss=1e10\n",
    "Final_Test_AUC_fromloss=0\n",
    "Final_Test_AP_fromloss=0\n",
    "\n",
    "Best_Val_fromAUC=0\n",
    "Final_Test_AUC_fromAUC=0\n",
    "Final_Test_AP_fromAUC=0\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    loss_epoch = train(train_loader,epoch)\n",
    "    val_auc, val_ap, val_loss,_ = test(val_loader,data_type='val')\n",
    "    #test_auc,test_ap,_,_ = test(test_loader,data_type='test')\n",
    "    if val_loss < Best_Val_fromloss:\n",
    "        Best_Val_fromloss = val_loss\n",
    "        #Final_Test_AUC_fromloss = test_auc\n",
    "        #Final_Test_AP_fromloss = test_ap\n",
    "\n",
    "    if val_auc > Best_Val_fromAUC:\n",
    "        Best_Val_fromAUC = val_auc\n",
    "        #Final_Test_AUC_fromAUC = test_auc\n",
    "        #Final_Test_AP_fromAUC = test_ap\n",
    "    print(f'Epoch: {epoch:03d}, Loss : {loss_epoch:.4f},\\\n",
    "     Val Loss : {val_loss:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    '''\n",
    "    print(f'Epoch: {epoch:03d}, Loss : {loss_epoch:.4f},\\\n",
    "     Val Loss : {val_loss:.4f}, Val AUC: {val_auc:.4f},\\\n",
    "      Test AUC: {test_auc:.4f}, Picked AUC:{Final_Test_AUC_fromAUC:.4f}')\n",
    "print(f'From loss: Final Test AUC: {Final_Test_AUC_fromloss:.4f}, Final Test AP: {Final_Test_AP_fromloss:.4f}')\n",
    "print(f'From AUC: Final Test AUC: {Final_Test_AUC_fromAUC:.4f}, Final Test AP: {Final_Test_AP_fromAUC:.4f}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed8e5ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test:: 100%|██████████| 68/68 [00:03<00:00, 20.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2172, 1)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "scores = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "loss_total=0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader,desc='test:'):  # Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "        out = out.cpu().clone().detach()\n",
    "        scores = torch.cat((scores,out),dim = 0)\n",
    "    scores = scores.cpu().clone().detach().numpy()\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a8eda224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0256681, 0.0256681, 0.0256681, ..., 0.0256681, 0.0256681,\n",
       "       0.0256681], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97c91cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E10559</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E4849</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3964</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E542</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E331</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2167</th>\n",
       "      <td>E2524</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2168</th>\n",
       "      <td>E4324</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2169</th>\n",
       "      <td>E1384</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2170</th>\n",
       "      <td>E7582</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171</th>\n",
       "      <td>E5209</td>\n",
       "      <td>0.025668</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2172 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id      prob\n",
       "0     E10559  0.025668\n",
       "1      E4849  0.025668\n",
       "2      E3964  0.025668\n",
       "3       E542  0.025668\n",
       "4       E331  0.025668\n",
       "...      ...       ...\n",
       "2167   E2524  0.025668\n",
       "2168   E4324  0.025668\n",
       "2169   E1384  0.025668\n",
       "2170   E7582  0.025668\n",
       "2171   E5209  0.025668\n",
       "\n",
       "[2172 rows x 2 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Upload['prob'] = scores.flatten().astype('float64')\n",
    "Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e14e0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upload.to_csv('output/WalkPooling_'+str(dss_num+1)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9736e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
