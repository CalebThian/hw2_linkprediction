{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2df931a",
   "metadata": {},
   "source": [
    "# Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9bdc5e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caleb\\anaconda3\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_undirected, from_scipy_sparse_matrix,dense_to_sparse,is_undirected\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import softmax\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Parameter,Embedding\n",
    "import torch.nn.functional as F\n",
    "from torch_scatter import scatter_mean, scatter, scatter_add, scatter_max\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b27941b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>E370</td>\n",
       "      <td>26</td>\n",
       "      <td>317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E667</td>\n",
       "      <td>196</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E3190</td>\n",
       "      <td>739</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E848</td>\n",
       "      <td>576</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E2161</td>\n",
       "      <td>466</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id   to  from\n",
       "0   E370   26   317\n",
       "1   E667  196   323\n",
       "2  E3190  739   468\n",
       "3   E848  576   156\n",
       "4  E2161  466   199"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"hw2_data/\"\n",
    "dss = ['dataset1','dataset2','dataset3'] #datasets\n",
    "dataset = dict()\n",
    "for ds in dss:\n",
    "    dataset[ds] = dict()\n",
    "    dataset[ds]['content'] = pd.read_csv(path+ds+\"/content.csv\",delimiter = '\\t',header = None)\n",
    "    dataset[ds]['train'] = pd.read_csv(path+ds+\"/train.csv\",delimiter = ',')\n",
    "    dataset[ds]['test'] = pd.read_csv(path+ds+\"/test.csv\",delimiter = ',')\n",
    "    dataset[ds]['upload'] = pd.read_csv(path+ds+\"/upload.csv\",delimiter = ',')\n",
    "dataset[dss[2]]['test'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef0da49",
   "metadata": {},
   "source": [
    "## Data Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "222d51e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2708, 1434)\n",
      "2708\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1433,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[dss[0]]['content'].shape)\n",
    "print(len(dataset[dss[0]]['content']))\n",
    "np.array(dataset[dss[0]]['content'].iloc[0,1:]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2563c847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8686, 4)\n",
      "8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2397"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(dataset[dss[0]]['train'].shape)\n",
    "print(len(dataset[dss[0]]['train']))\n",
    "dataset[dss[0]]['train'].loc[1,'to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c400784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edge(0,1,label=0)\n",
    "G.add_edge(0,2,label=1)\n",
    "G.add_edge(1,2,label=1)\n",
    "nx.draw(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54b7b149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2, 1), (1, 2, 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in list(G.edges.data('label')) if e[2]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6401abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For example:\n",
    "Content = dataset[dss[0]]['content']\n",
    "Train = dataset[dss[0]]['train']\n",
    "Test = dataset[dss[0]]['test']\n",
    "Upload = dataset[dss[0]]['upload']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbb0319c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2708, 1434)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "af37fe37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>to</th>\n",
       "      <th>from</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>E10087</td>\n",
       "      <td>1767</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>E2206</td>\n",
       "      <td>2062</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5178</th>\n",
       "      <td>E783</td>\n",
       "      <td>455</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5845</th>\n",
       "      <td>E1142</td>\n",
       "      <td>1547</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    to  from  label\n",
       "14    E10087  1767     4      1\n",
       "1698   E2206  2062     4      1\n",
       "5178    E783   455     4      0\n",
       "5845   E1142  1547     4      1"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train[Train['from']==4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b5e9f",
   "metadata": {},
   "source": [
    "---\n",
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3980e325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da085f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cd7d9e",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9327be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def floor(x):\n",
    "    return torch.div(x, 1, rounding_mode='trunc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44349bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_features(features):\n",
    "    # Row-normalize feature matrix and convert to tuple representation\n",
    "    # Because the datasets only have binary features, row-normalize is unnecessary\n",
    "    rowsum = np.array(features.sum(1),dtype = np.float32)\n",
    "    rowsum = (rowsum==0)*1+rowsum\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d2e8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(content,train, test):\n",
    "    G = nx.DiGraph()\n",
    "    G_test = nx.DiGraph()\n",
    "    # for easier split the edges, create 2 graph, 1 with positive edge, the other with given negative edges\n",
    "    G_pos = nx.DiGraph()\n",
    "    G_neg = nx.DiGraph()\n",
    "    graph_node_features_dict = dict()\n",
    "    \n",
    "    for i in range(len(content)):\n",
    "        #graph_node_features_dict[content.iloc[i,0]] = np.array(content.iloc[i,1:])\n",
    "        G.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        G_test.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        # pos and neg\n",
    "        G_pos.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        G_neg.add_node(int(content.iloc[i,0]),features = np.array(content.iloc[i,1:]))\n",
    "        \n",
    "    for i in range(len(train)):\n",
    "        # Adding nodes into G\n",
    "        '''\n",
    "        if train.loc[i,'from'] not in G:\n",
    "            G.add_node(train.loc[i,'from'],features = graph_node_features_dict[train.loc[i,'from']])\n",
    "        if train.loc[i,'to'] not in G:\n",
    "            G.add_node(train.loc[i,'to'],features = graph_node_features_dict[train.loc[i,'to']])\n",
    "        ''' \n",
    "        # Adding edges\n",
    "        G.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "        \n",
    "        # pos and neg\n",
    "        if train.loc[i,'label'] == 0: \n",
    "            G_neg.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "        else:\n",
    "            G_pos.add_edge(train.loc[i,'from'],train.loc[i,'to'],label = train.loc[i,'label'])\n",
    "    \n",
    "    for i in range(len(test)):\n",
    "        # Adding edges\n",
    "        G_test.add_edge(test.loc[i,'from'],test.loc[i,'to'])\n",
    "    \n",
    "    adj = nx.adjacency_matrix(G,sorted(G.nodes()))\n",
    "    adj_test = nx.adjacency_matrix(G_test,sorted(G_test.nodes()))\n",
    "    adj_pos = nx.adjacency_matrix(G_pos,sorted(G_pos.nodes()))\n",
    "    adj_neg = nx.adjacency_matrix(G_neg,sorted(G_neg.nodes()))\n",
    "    features = np.array(\n",
    "        [features for _, features in sorted(G.nodes(data='features'), key=lambda x: x[0])])\n",
    "    features = preprocess_features(features)\n",
    "    \n",
    "    # Skip train,valid,and test mask\n",
    "    \n",
    "    num_features = features.shape[1]\n",
    "    features = torch.FloatTensor(features)\n",
    "    return adj, adj_test, adj_pos, adj_neg, features, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56e6b914",
   "metadata": {},
   "outputs": [],
   "source": [
    "g, gtest, pos, neg, features, num_features = load_data(Content,Train,Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a433235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges(data):\n",
    "    set_random_seed(10) #Seed is randomly set\n",
    "    \n",
    "    # ? This select the edges that from<to, but why? Don't understand, not using this\n",
    "    row, col = data.edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask], col[mask]\n",
    "    \n",
    "    val_ratio = 0.1\n",
    "    #test_ratio = 0.1\n",
    "    \n",
    "    #split positive edges\n",
    "    row_pos, col_pos = data.pos\n",
    "    mask_pos = row_pos < col_pos\n",
    "    row_pos, col_pos = row_pos[mask_pos], col_pos[mask_pos]\n",
    "\n",
    "    n_v = floor(val_ratio * row_pos.size(0)).int() #number of validation positive edges\n",
    "    #n_t = floor(test_ratio * row_pos.size(0)).int() #number of test positive edges\n",
    "    \n",
    "    perm = torch.randperm(row_pos.size(0))\n",
    "    row_pos, col_pos = row_pos[perm], col_pos[perm]\n",
    "    r, c = row_pos[:n_v], col_pos[:n_v]\n",
    "    data.val_pos = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_pos[n_v:n_v+n_t], col_pos[n_v:n_v+n_t]\n",
    "    #data.test_pos = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_pos[n_v+n_t:], col_pos[n_v+n_t:]\n",
    "    r, c = row_pos[n_v:], col_pos[n_v:]\n",
    "    data.train_pos = torch.stack([r, c], dim=0)\n",
    "    print(\"Positive edges shape(train,val):\",end=\"\")\n",
    "    #print(data.train_pos.shape,data.test_pos.shape,data.val_pos.shape)\n",
    "    print(data.train_pos.shape,data.val_pos.shape)\n",
    "    \n",
    "    #split neg edges\n",
    "    row_neg, col_neg = data.neg\n",
    "    mask_neg = row_neg < col_neg\n",
    "    row_neg, col_neg = row_neg[mask_neg], col_neg[mask_neg]\n",
    "    \n",
    "    \n",
    "    n_v = floor(val_ratio * row_neg.size(0)).int() #number of validation positive edges\n",
    "    #n_t = floor(test_ratio * row_neg.size(0)).int() #number of test positive edges\n",
    "    \n",
    "    perm = torch.randperm(row_neg.size(0))\n",
    "    row_neg, col_neg = row_neg[perm], col_neg[perm]\n",
    "    r, c = row_neg[:n_v], col_neg[:n_v]\n",
    "    data.val_neg = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_neg[n_v:n_v+n_t], col_neg[n_v:n_v+n_t]\n",
    "    #data.test_neg = torch.stack([r, c], dim=0)\n",
    "    #r, c = row_neg[n_v+n_t:], col_neg[n_v+n_t:]\n",
    "    r, c = row_neg[n_v:], col_neg[n_v:]\n",
    "    data.train_neg = torch.stack([r, c], dim=0)\n",
    "    print(\"Negative edges shape(train,val):\",end=\"\")\n",
    "    #print(data.train_neg.shape,data.test_neg.shape,data.val_neg.shape)\n",
    "    print(data.train_neg.shape,data.val_neg.shape)\n",
    "\n",
    "    #costruct real test edges\n",
    "    row_test,col_test = data.test\n",
    "    data.test = torch.stack([row_test,col_test],dim = 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bebf0d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_init_attribute_representation(data):\n",
    "    # Construct data_observed and compute its node attributes & representation\n",
    "    # Why create undirected edge? Don't use this\n",
    "    # Now pretend args.observe_val_and_injection = False\n",
    "    #print('Here')\n",
    "    #print(data.train_pos.shape,data.train_pos[[1,0],:].shape)\n",
    "    #print(data.train_pos,data.train_pos[[1,0],:])\n",
    "    #print(data.train_pos[:,:],,data.train_pos[[1,0],:])\n",
    "    edge_index = torch.cat((data.train_pos,data.train_pos[[1,0],:]),dim=1)\n",
    "    #print(edge_index)\n",
    "    #edge_index = data.train_pos\n",
    "    #edge_index=torch.cat((edge_index,data.val_pos,data.val_pos[[1,0],:]),dim=1)\n",
    "    data_observed = Data(edge_index=edge_index)\n",
    "    data_observed.num_nodes = data.num_nodes\n",
    "    #use the injection trick and add val data in observed graph \n",
    "    #edge_index_observed = torch.cat((data_observed.edge_index,\\\n",
    "    #    data.train_neg,data.train_neg[[1,0],:],data.val_neg,data.val_neg[[1,0],:]),dim=1)\n",
    "    edge_index_observed = data_observed.edge_index\n",
    "  \n",
    "    #generate the initial node attribute if there isn't any\n",
    "    init_attribute = 'n2v' # Can choose between 'n2v','one_hot','spc','ones','zeros',\n",
    "    # But we already prepare features so it doesn't matter\n",
    "    if data.x == None:\n",
    "        if init_attribute =='n2v':\n",
    "            from node2vec import CalN2V\n",
    "            x = CalN2V(edge_index_observed)#,args)\n",
    "        if init_attribute =='one_hot':\n",
    "            x = F.one_hot(torch.arange(data.num_nodes), num_classes=data.num_nodes)\n",
    "            x = x.float()\n",
    "        if init_attribute =='spc':\n",
    "            from SPC import spc\n",
    "            x = spc(edge_index_observed)#,args)\n",
    "            x = x.float()\n",
    "        if init_attribute =='ones':\n",
    "            embedding_dim = 32\n",
    "            x = torch.ones(data.num_nodes,args.embedding_dim)\n",
    "            x = x.float()\n",
    "        if init_attribute =='zeros':\n",
    "            embedding_dim = 32\n",
    "            x = torch.zeros(data.num_nodes,args.embedding_dim)\n",
    "            x = x.float()\n",
    "    else:\n",
    "        x = data.x\n",
    "    #generate the initial node representation using unsupervised models\n",
    "    \n",
    "    init_representation = None # Don't understand what this part doing\n",
    "    if init_representation != None:\n",
    "        val_and_test=[data.test_pos,data.test_neg,data.val_pos,data.val_neg]\n",
    "        num_nodes,_=x.shape\n",
    "        #add self-loop for the last node to aviod losing node if the last node dosen't have a link.\n",
    "        if (num_nodes-1) in edge_index_observed:\n",
    "            edge_index_observed=edge_index_observed.clone().detach()\n",
    "        else:\n",
    "            edge_index_observed=torch.cat((edge_index_observed.clone().detach(),torch.tensor([[num_nodes-1],[num_nodes-1]])),dim=1)\n",
    "        if init_representation == 'gic':\n",
    "            par_dir = os.path.abspath(os.path.join(os.path.dirname(__file__),\"..\"))\n",
    "            sys.path.append('%s/software/GIC/' % args.par_dir)\n",
    "            from GICEmbs import CalGIC\n",
    "            data_observed.x, auc, ap = CalGIC(edge_index_observed, x, args.data_name, val_and_test,args)\n",
    "\n",
    "        if init_representation == 'vgae':\n",
    "            from vgae import CalVGAE\n",
    "            data_observed.x, auc, ap = CalVGAE(edge_index_observed, x, val_and_test, args)\n",
    "        if init_representation == 'svgae':\n",
    "            from svgae import CalSVGAE\n",
    "            data_observed.x, auc, ap = CalSVGAE(edge_index_observed, x, val_and_test, args)\n",
    "        if init_representation == 'argva':\n",
    "            from argva import CalARGVA\n",
    "            data_observed.x, auc, ap = CalARGVA(edge_index_observed, x, val_and_test, args)\n",
    "        feature_results=[auc,ap]\n",
    "    else:\n",
    "        data_observed.x = x\n",
    "        feature_results=None\n",
    "\n",
    "    return data_observed,feature_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce585441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_hop_subgraph(node_idx, num_hops, edge_index, max_nodes_per_hop = None,num_nodes = None):\n",
    "  \n",
    "    if num_nodes == None:\n",
    "        num_nodes = torch.max(edge_index)+1\n",
    "    row, col = edge_index\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    node_idx = node_idx.to(row.device)\n",
    "\n",
    "    subsets = [node_idx]\n",
    "\n",
    "    if max_nodes_per_hop == None:\n",
    "        for _ in range(num_hops):\n",
    "            node_mask.fill_(False)\n",
    "            node_mask[subsets[-1]] = True\n",
    "            torch.index_select(node_mask, 0, row, out = edge_mask)\n",
    "            subsets.append(col[edge_mask])\n",
    "    else:\n",
    "        not_visited = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "        not_visited.fill_(True)\n",
    "        for _ in range(num_hops):\n",
    "            node_mask.fill_(False)# the source node mask in this hop\n",
    "            node_mask[subsets[-1]] = True #mark the sources\n",
    "            not_visited[subsets[-1]] = False # mark visited nodes\n",
    "            torch.index_select(node_mask, 0, row, out = edge_mask) # indices of all neighbors\n",
    "            neighbors = col[edge_mask].unique() #remove repeats\n",
    "            neighbor_mask = row.new_empty(num_nodes, dtype=torch.bool) # mask of all neighbor nodes\n",
    "            edge_mask_hop = row.new_empty(row.size(0), dtype=torch.bool) # selected neighbor mask in this hop\n",
    "            neighbor_mask.fill_(False)\n",
    "            neighbor_mask[neighbors] = True\n",
    "            neighbor_mask = torch.logical_and(neighbor_mask, not_visited) # all neighbors that are not visited\n",
    "            ind = torch.where(neighbor_mask==True) #indicies of all the unvisited neighbors\n",
    "            if ind[0].size(0) > max_nodes_per_hop:\n",
    "                perm = torch.randperm(ind[0].size(0))\n",
    "                ind = ind[0][perm]\n",
    "                neighbor_mask[ind[max_nodes_per_hop:]] = False # randomly select max_nodes_per_hop nodes\n",
    "                torch.index_select(neighbor_mask, 0, col, out = edge_mask_hop)# find the indicies of selected nodes\n",
    "                edge_mask = torch.logical_and(edge_mask,edge_mask_hop) # change edge_mask\n",
    "            subsets.append(col[edge_mask])\n",
    "\n",
    "    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
    "    inv = inv[:node_idx.numel()]\n",
    "\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "    edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    node_idx = row.new_full((num_nodes, ), -1)\n",
    "    node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "    edge_index = node_idx[edge_index]\n",
    "\n",
    "    return subset, edge_index, inv, edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac382c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plus_edge(data_observed, label, p_edge):\n",
    "    num_hops = 2 # number of hops in sampling subgraph, set 2 which same as default\n",
    "    max_nodes_per_hop =  None #when the graph is too large or too dense, \n",
    "    #we need max node per hop threshold to avoid OOM. Default is None\n",
    "    nodes, edge_index_m, mapping, _ = k_hop_subgraph(node_idx= p_edge, num_hops= num_hops,\\\n",
    " edge_index = data_observed.edge_index, max_nodes_per_hop= max_nodes_per_hop ,num_nodes=data_observed.num_nodes)\n",
    "    x_sub = data_observed.x[nodes,:]\n",
    "    edge_index_p = edge_index_m\n",
    "    edge_index_p = torch.cat((edge_index_p, mapping.view(-1,1)),dim=1)\n",
    "    edge_index_p = torch.cat((edge_index_p, mapping[[1,0]].view(-1,1)),dim=1)\n",
    "\n",
    "    #edge_mask marks the edge under perturbation, i.e., the candidate edge for LP\n",
    "    edge_mask = torch.ones(edge_index_p.size(1),dtype=torch.bool)\n",
    "    edge_mask[-1] = False\n",
    "    edge_mask[-2] = False\n",
    "    \n",
    "    '''\n",
    "    # Run here for drnl labeling\n",
    "    if drnl == True:\n",
    "        num_nodes = torch.max(edge_index_p)+1\n",
    "        z = drnl_node_labeling(edge_index_m, mapping[0],mapping[1],num_nodes)\n",
    "        data = Data(edge_index = edge_index_p, x = x_sub, z = z)\n",
    "    '''\n",
    "    data = Data(edge_index = edge_index_p, x = x_sub, z = 0)\n",
    "    data.edge_mask = edge_mask\n",
    "\n",
    "    #label = 1 if the candidate link (p_edge) is positive and label=0 otherwise\n",
    "    data.label = float(label)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59e2764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minus_edge(data_observed, label, p_edge):\n",
    "    num_hops = 2 # number of hops in sampling subgraph, set 2 which same as default\n",
    "    max_nodes_per_hop =  None #when the graph is too large or too dense, \n",
    "    #we need max node per hop threshold to avoid OOM. Default is None\n",
    "    \n",
    "    nodes, edge_index_p, mapping,_ = k_hop_subgraph(node_idx= p_edge, num_hops=num_hops,\\\n",
    " edge_index = data_observed.edge_index,max_nodes_per_hop=max_nodes_per_hop, num_nodes = data_observed.num_nodes)\n",
    "    x_sub = data_observed.x[nodes,:]\n",
    "\n",
    "    #edge_mask marks the edge under perturbation, i.e., the candidate edge for LP\n",
    "    edge_mask = torch.ones(edge_index_p.size(1), dtype = torch.bool)\n",
    "    ind = torch.where((edge_index_p == mapping.view(-1,1)).all(dim=0))\n",
    "    edge_mask[ind[0]] = False\n",
    "    ind = torch.where((edge_index_p == mapping[[1,0]].view(-1,1)).all(dim=0))\n",
    "    edge_mask[ind[0]] = False\n",
    "    '''\n",
    "    if args.drnl == True:\n",
    "        num_nodes = torch.max(edge_index_p)+1\n",
    "        z = drnl_node_labeling(edge_index_p[:,edge_mask], mapping[0],mapping[1],num_nodes)\n",
    "        data = Data(edge_index = edge_index_p, x= x_sub,z = z)\n",
    "    else:\n",
    "    '''\n",
    "    data = Data(edge_index = edge_index_p, x= x_sub,z = 0)\n",
    "    data.edge_mask = edge_mask\n",
    "\n",
    "    #label = 1 if the candidate link (p_edge) is positive and label=0 otherwise\n",
    "    data.label = float(label)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbe9c704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "def prepare_data(content,train,test):\n",
    "    g, gtest, pos, neg, features, num_features = load_data(content,train,test)\n",
    "    A = g.toarray()\n",
    "    edge_index,_ = dense_to_sparse(torch.tensor(A))\n",
    "    Atest = gtest.toarray()\n",
    "    edge_index_test,_ = dense_to_sparse(torch.tensor(Atest))\n",
    "    Apos = pos.toarray()\n",
    "    edge_index_pos,_ = dense_to_sparse(torch.tensor(Apos))\n",
    "    Aneg = neg.toarray()\n",
    "    edge_index_neg,_ = dense_to_sparse(torch.tensor(Aneg))\n",
    "    data = Data(edge_index=edge_index,x=features.to(torch.float),\n",
    "                pos = edge_index_pos,neg = edge_index_neg,test = edge_index_test)\n",
    "    data = split_edges(data)\n",
    "    \n",
    "\n",
    "    set_random_seed(42) # Random set the seed\n",
    "    data_observed,feature_results= set_init_attribute_representation(data)\n",
    "    # Construct train, val and test data loader\n",
    "    set_random_seed(42)\n",
    "    train_graphs = []\n",
    "    val_graphs = []\n",
    "    test_graphs = []\n",
    "    for i in range(data.train_pos.size(1)):\n",
    "        train_graphs.append(minus_edge(data_observed,1,data.train_pos[:,i]))\n",
    "\n",
    "    for i in range(data.train_neg.size(1)):\n",
    "        train_graphs.append(plus_edge(data_observed,0,data.train_neg[:,i]))\n",
    "    '''\n",
    "    for i in range(data.test_pos.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,1,data.test_pos[:,i]))\n",
    "\n",
    "    for i in range(data.test_neg.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,0,data.test_neg[:,i]))   \n",
    "    '''\n",
    "    for i in range(data.test.size(1)):\n",
    "        test_graphs.append(plus_edge(data_observed,1,data.test[:,i]))\n",
    "    \n",
    "    #if args.observe_val_and_injection == False:\n",
    "    # pretend args.observe_val_and_injection == False\n",
    "    for i in range(data.val_pos.size(1)):\n",
    "        val_graphs.append(plus_edge(data_observed,1,data.val_pos[:,i]))\n",
    "\n",
    "    for i in range(data.val_neg.size(1)):\n",
    "        val_graphs.append(plus_edge(data_observed,0,data.val_neg[:,i]))\n",
    "    '''\n",
    "    else:\n",
    "        for i in range(data.val_pos.size(1)):\n",
    "            val_graphs.append(minus_edge(data_observed,1,data.val_pos[:,i]))\n",
    "\n",
    "        for i in range(data.val_neg.size(1)):\n",
    "            val_graphs.append(plus_edge(data_observed,0,data.val_neg[:,i]))\n",
    "    '''\n",
    "\n",
    "    \n",
    "    print('Train_link:', str(len(train_graphs)),\n",
    "          ' Val_link:',str(len(val_graphs)),' Test_link:',str(len(test_graphs)))\n",
    "\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_graphs,batch_size=batch_size,shuffle=True)\n",
    "    val_loader = DataLoader(val_graphs,batch_size=batch_size,shuffle=True)\n",
    "    test_loader = DataLoader(test_graphs,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader,feature_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b992a8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive edges shape(train,val):torch.Size([2, 1945]) torch.Size([2, 216])\n",
      "Negative edges shape(train,val):torch.Size([2, 1882]) torch.Size([2, 209])\n",
      "Train_link: 3827  Val_link: 425  Test_link: 2172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch_geometric.loader.dataloader.DataLoader at 0x1f3da463408>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader,feature_results = prepare_data(Content,Train,Test)\n",
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c71d7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d59a72d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1433"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features = next(iter(train_loader)).x.size(1)\n",
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7dd565f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimention of features after concatenation: 1433\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimention of features after concatenation:\",num_features)\n",
    "set_random_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e3351",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aeb948f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3995fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPred(MessagePassing):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, heads: int = 1,\\\n",
    "                 walk_len: int = 6, drnl: bool = False, z_max: int =100, MSE: bool=True):\n",
    "        super(LinkPred, self).__init__()\n",
    "\n",
    "        self.drnl = drnl\n",
    "        if drnl == True:\n",
    "            self.z_embedding = Embedding(z_max, hidden_channels)\n",
    "\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "        self.wp = WalkPooling(in_channels + hidden_channels*2,\\\n",
    "            hidden_channels, heads, walk_len)\n",
    "\n",
    "        L=walk_len*5+1\n",
    "        self.classifier = MLP(L*heads,MSE=MSE)\n",
    "        self.norm = nn.BatchNorm1d(hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_mask, batch, z = None):\n",
    "        \n",
    "        #using drnl\n",
    "        if self.drnl == True:\n",
    "            z_emb = self.z_embedding(z)\n",
    "            if z_emb.ndim == 3:  # in case z has multiple integer labels\n",
    "                z_emb = z_emb.sum(dim=1)\n",
    "            z_emb = z_emb.view(x.size(0),-1)\n",
    "            x = torch.cat((x,z_emb.view(x.size(0),-1)),dim=1)\n",
    "        \n",
    "        #GCN layers\n",
    "        x_out = x\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x_out = torch.cat((x_out,x),dim=1)\n",
    "        x = x.relu()\n",
    "        x = self.norm(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.norm(x)\n",
    "        x_out = torch.cat((x_out,x),dim=1)\n",
    "\n",
    "        #Walk Pooling\n",
    "        feature_list = self.wp(x_out, edge_index, edge_mask, batch)\n",
    "\n",
    "        #Classifier\n",
    "        out = self.classifier(feature_list)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class WalkPooling(MessagePassing):\n",
    "    def __init__(self, in_channels: int, hidden_channels: int, heads: int = 1,\\\n",
    "                 walk_len: int = 6):\n",
    "        super(WalkPooling, self).__init__()\n",
    "\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.heads = heads\n",
    "        self.walk_len = walk_len\n",
    "\n",
    "        # the linear layers in the attention encoder\n",
    "        self.lin_key1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin_query1 = Linear(in_channels, hidden_channels)\n",
    "        self.lin_key2 = Linear(hidden_channels, heads * hidden_channels)\n",
    "        self.lin_query2 = Linear(hidden_channels, heads * hidden_channels)\n",
    "    def attention_mlp(self, x, edge_index):\n",
    "    \n",
    "        query = self.lin_key1(x).reshape(-1,self.hidden_channels)\n",
    "        key = self.lin_query1(x).reshape(-1,self.hidden_channels)\n",
    "\n",
    "        query = F.leaky_relu(query,0.2)\n",
    "        key = F.leaky_relu(key,0.2)\n",
    "\n",
    "        query = F.dropout(query, p=0.5, training=self.training)\n",
    "        key = F.dropout(key, p=0.5, training=self.training)\n",
    "\n",
    "        query = self.lin_key2(query).view(-1, self.heads, self.hidden_channels)\n",
    "        key = self.lin_query2(key).view(-1, self.heads, self.hidden_channels)\n",
    "\n",
    "        row, col = edge_index\n",
    "        weights = (query[row] * key[col]).sum(dim=-1) / np.sqrt(self.hidden_channels)\n",
    "        \n",
    "        return weights\n",
    "\n",
    "    def weight_encoder(self, x, edge_index, edge_mask):        \n",
    "     \n",
    "        weights = self.attention_mlp(x, edge_index)\n",
    "    \n",
    "        omega = torch.sigmoid(weights[torch.logical_not(edge_mask)])\n",
    "        \n",
    "        row, col = edge_index\n",
    "        num_nodes = torch.max(edge_index)+1\n",
    "\n",
    "        # edge weights of the plus graph\n",
    "        weights_p = softmax(weights,edge_index[1])\n",
    "\n",
    "        # edge weights of the minus graph\n",
    "        weights_m = weights - scatter_max(weights, col, dim=0, dim_size=num_nodes)[0][col]\n",
    "        weights_m = torch.exp(weights_m)\n",
    "        weights_m = weights_m * edge_mask.view(-1,1)\n",
    "        norm = scatter_add(weights_m, col, dim=0, dim_size=num_nodes)[col] + 1e-16\n",
    "        weights_m = weights_m / norm\n",
    "\n",
    "        return weights_p, weights_m, omega\n",
    "\n",
    "    def forward(self, x, edge_index, edge_mask, batch):\n",
    "        \n",
    "        #encode the node representation into edge weights via attention mechanism\n",
    "        weights_p, weights_m, omega = self.weight_encoder(x, edge_index, edge_mask)\n",
    "\n",
    "        # pytorch geometric set the batch adjacency matrix to\n",
    "        # be the diagonal matrix with each graph's adjacency matrix\n",
    "        # stacked in the diagonal. Therefore, calculating the powers\n",
    "        # of the stochastic matrix directly will cost lots of memory.\n",
    "        # We compute the powers of stochastic matrix as follows\n",
    "        # Let A = diag ([A_1,\\cdots,A_n]) be the batch adjacency matrix,\n",
    "        # we set x = [x_1,\\cdots,x_n]^T be the batch feature matrix\n",
    "        # for the i-th graph in the batch with n_i nodes, its feature \n",
    "        # is a n_i\\times n_max matrix, where n_max is the largest number\n",
    "        # of nodes for all graphs in the batch. The elements of x_i are\n",
    "        # (x_i)_{x,y} = \\delta_{x,y}. \n",
    "\n",
    "        # number of graphs in the batch\n",
    "        batch_size = torch.max(batch)+1\n",
    "\n",
    "        # for node i in the batched graph, index[i] is i's id in the graph before batch \n",
    "        index = torch.zeros(batch.size(0),1,dtype=torch.long)\n",
    "        \n",
    "        # numer of nodes in each graph\n",
    "        _, counts = torch.unique(batch, sorted=True, return_counts=True)\n",
    "        \n",
    "        # maximum number of nodes for all graphs in the batch\n",
    "        max_nodes = torch.max(counts)\n",
    "\n",
    "        # set the values in index\n",
    "        id_start = 0\n",
    "        for i in range(batch_size):\n",
    "            index[id_start:id_start+counts[i]] = torch.arange(0,counts[i],dtype=torch.long).view(-1,1)\n",
    "            id_start = id_start+counts[i]\n",
    "\n",
    "        index = index.to(device)\n",
    "        \n",
    "        #the output graph features of walk pooling\n",
    "        nodelevel_p = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        nodelevel_m = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        linklevel_p = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        linklevel_m = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        graphlevel = torch.zeros(batch_size,(self.walk_len*self.heads)).to(device)\n",
    "        # a link (i,j) has two directions i->j and j->i, and\n",
    "        # when extract the features of the link, we usually average over\n",
    "        # the two directions. indices_odd and indices_even records the\n",
    "        # indices for a link in two directions\n",
    "        indices_odd = torch.arange(0,omega.size(0),2).to(device)\n",
    "        indices_even = torch.arange(1,omega.size(0),2).to(device)\n",
    "\n",
    "        omega = torch.index_select(omega, 0 ,indices_even)\\\n",
    "        + torch.index_select(omega,0,indices_odd)\n",
    "        \n",
    "        #node id of the candidate (or perturbation) link\n",
    "        link_ij, link_ji = edge_index[:,torch.logical_not(edge_mask)]\n",
    "        node_i = link_ij[indices_odd]\n",
    "        node_j = link_ij[indices_even]\n",
    "\n",
    "        # compute the powers of stochastic matrix\n",
    "        for head in range(self.heads):\n",
    "\n",
    "            # x on the plus graph and minus graph\n",
    "            x_p = torch.zeros(batch.size(0),max_nodes,dtype=x.dtype).to(device)\n",
    "            x_p = x_p.scatter_(1,index,1)\n",
    "            x_m = torch.zeros(batch.size(0),max_nodes,dtype=x.dtype).to(device)\n",
    "            x_m = x_m.scatter_(1,index,1)\n",
    "\n",
    "            # propagage once\n",
    "            x_p = self.propagate(edge_index, x= x_p, norm = weights_p[:,head])\n",
    "            x_m = self.propagate(edge_index, x= x_m, norm = weights_m[:,head])\n",
    "        \n",
    "            # start from tau = 2\n",
    "            for i in range(self.walk_len):\n",
    "                #print(f\"self.walk_len:{i}\")\n",
    "                x_p = self.propagate(edge_index, x= x_p, norm = weights_p[:,head])\n",
    "                x_m = self.propagate(edge_index, x= x_m, norm = weights_m[:,head])\n",
    "                \n",
    "                # returning probabilities around i + j\n",
    "                nodelevel_p_w = x_p[node_i,index[node_i].view(-1)] + x_p[node_j,index[node_j].view(-1)]\n",
    "                nodelevel_m_w = x_m[node_i,index[node_i].view(-1)] + x_m[node_j,index[node_j].view(-1)]\n",
    "                nodelevel_p[:,head*self.walk_len+i] = nodelevel_p_w.view(-1)\n",
    "                nodelevel_m[:,head*self.walk_len+i] = nodelevel_m_w.view(-1)\n",
    "  \n",
    "                # transition probabilities between i and j\n",
    "                linklevel_p_w = x_p[node_i,index[node_j].view(-1)] + x_p[node_j,index[node_i].view(-1)]\n",
    "                linklevel_m_w = x_m[node_i,index[node_j].view(-1)] + x_m[node_j,index[node_i].view(-1)]\n",
    "                linklevel_p[:,head*self.walk_len+i] = linklevel_p_w.view(-1)\n",
    "                linklevel_m[:,head*self.walk_len+i] = linklevel_m_w.view(-1)\n",
    "\n",
    "                # graph average of returning probabilities\n",
    "                diag_ele_p = torch.gather(x_p,1,index)\n",
    "                diag_ele_m = torch.gather(x_m,1,index)\n",
    "\n",
    "                graphlevel_p = scatter_add(diag_ele_p, batch, dim = 0)\n",
    "                graphlevel_m = scatter_add(diag_ele_m, batch, dim = 0)\n",
    "\n",
    "                graphlevel[:,head*self.walk_len+i] = (graphlevel_p-graphlevel_m).view(-1)\n",
    "         \n",
    "        feature_list = graphlevel \n",
    "        feature_list = torch.cat((feature_list,omega),dim=1)\n",
    "        feature_list = torch.cat((feature_list,nodelevel_p),dim=1)\n",
    "        feature_list = torch.cat((feature_list,nodelevel_m),dim=1)\n",
    "        feature_list = torch.cat((feature_list,linklevel_p),dim=1)\n",
    "        feature_list = torch.cat((feature_list,linklevel_m),dim=1)\n",
    "\n",
    "\n",
    "        return feature_list\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j  \n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    # adopt a MLP as classifier for graphs\n",
    "    def __init__(self,input_size,MSE=True):\n",
    "        super(MLP, self).__init__()\n",
    "        self.nn = nn.BatchNorm1d(input_size)\n",
    "        self.linear1 = torch.nn.Linear(input_size,input_size*20)\n",
    "        self.linear2 = torch.nn.Linear(input_size*20,input_size*20)\n",
    "        self.linear3 = torch.nn.Linear(input_size*20,input_size*10)\n",
    "        self.linear4 = torch.nn.Linear(input_size*10,input_size)\n",
    "        self.linear5 = torch.nn.Linear(input_size,1)\n",
    "        self.act= nn.ReLU()\n",
    "        self.MSE=MSE\n",
    "        self.norm1 = nn.BatchNorm1d(input_size*20)\n",
    "        self.norm2 = nn.BatchNorm1d(input_size*20)\n",
    "        self.norm3 = nn.BatchNorm1d(input_size*10)\n",
    "        self.norm4 = nn.BatchNorm1d(input_size)\n",
    "    def forward(self, x):\n",
    "        out= self.nn(x)\n",
    "        out= self.linear1(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm1(out)\n",
    "        out= self.linear2(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.linear3(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm3(out)\n",
    "        out = self.linear4(out)\n",
    "        out = self.act(out)\n",
    "        out = self.norm4(out)\n",
    "        out = F.dropout(out, p=0.5, training=self.training)\n",
    "        out = self.linear5(out)\n",
    "        #out = torch.sigmoid(out)\n",
    "        if self.MSE:\n",
    "            out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bd176aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(loader,epoch):\n",
    "    model.train()\n",
    "    loss_epoch=0\n",
    "    for data in tqdm(loader,desc=\"train\"):  # Iterate in batches over the training dataset.\n",
    "        data = data.to(device)\n",
    "        label= data.label\n",
    "        out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "        torch.cuda.empty_cache()\n",
    "        loss = criterion(out.view(-1), label)  \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  \n",
    "        optimizer.step()\n",
    "        loss_epoch=loss_epoch+loss.item()\n",
    "    return loss_epoch/len(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c6c7d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(loader,data_type='test'):\n",
    "    model.eval()\n",
    "    scores = torch.tensor([])\n",
    "    labels = torch.tensor([])\n",
    "    loss_total=0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader,desc='test:'+data_type):  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "            loss = criterion(out.view(-1), data.label)\n",
    "            out = out.cpu().clone().detach()\n",
    "            scores = torch.cat((scores,out),dim = 0)\n",
    "            labels = torch.cat((labels,data.label.view(-1,1).cpu().clone().detach()),dim = 0)\n",
    "        scores = scores.cpu().clone().detach().numpy()\n",
    "        labels = labels.cpu().clone().detach().numpy()\n",
    "        loss_total=loss_total+loss.item()\n",
    "        return roc_auc_score(labels, scores), average_precision_score(labels, scores),loss_total,scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fe26ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_len = 10\n",
    "lr = 0.005\n",
    "heads = 5\n",
    "hidden_channels = 128\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "z_max = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f2c86da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinkPred(in_channels = num_features, hidden_channels = hidden_channels,\\\n",
    "    heads = heads, walk_len = walk_len, drnl = False,z_max = z_max, MSE= True).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr,weight_decay=0.005)\n",
    "criterion = torch.nn.MSELoss(reduction='mean')\n",
    "#criterion = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "040d522d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.20it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss : 0.2428,     Val Loss : 0.3251, Val AUC: 0.6487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.36it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 15.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss : 0.2533,     Val Loss : 0.1250, Val AUC: 0.5699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:19<00:00,  6.08it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 17.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss : 0.2513,     Val Loss : 0.2070, Val AUC: 0.6848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.95it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss : 0.2501,     Val Loss : 0.1951, Val AUC: 0.6525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.08it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss : 0.2404,     Val Loss : 0.2323, Val AUC: 0.6440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.26it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss : 0.2431,     Val Loss : 0.3108, Val AUC: 0.6593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.12it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006, Loss : 0.2382,     Val Loss : 0.2413, Val AUC: 0.6160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.20it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007, Loss : 0.2392,     Val Loss : 0.2805, Val AUC: 0.6423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.31it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008, Loss : 0.2408,     Val Loss : 0.2600, Val AUC: 0.6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.82it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009, Loss : 0.2397,     Val Loss : 0.1888, Val AUC: 0.6397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.81it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010, Loss : 0.2382,     Val Loss : 0.2104, Val AUC: 0.6301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.45it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011, Loss : 0.2322,     Val Loss : 0.2198, Val AUC: 0.6424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.72it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012, Loss : 0.2339,     Val Loss : 0.2570, Val AUC: 0.5632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.55it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 16.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013, Loss : 0.2386,     Val Loss : 0.2155, Val AUC: 0.6539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.08it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 18.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014, Loss : 0.2372,     Val Loss : 0.2417, Val AUC: 0.6363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.48it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 19.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 015, Loss : 0.2377,     Val Loss : 0.2000, Val AUC: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.97it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 016, Loss : 0.2396,     Val Loss : 0.2478, Val AUC: 0.6527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  7.01it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 017, Loss : 0.2354,     Val Loss : 0.2375, Val AUC: 0.6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.36it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 018, Loss : 0.2349,     Val Loss : 0.2683, Val AUC: 0.6665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.25it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 019, Loss : 0.2353,     Val Loss : 0.2131, Val AUC: 0.6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.07it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 020, Loss : 0.2351,     Val Loss : 0.3309, Val AUC: 0.6811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.89it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 021, Loss : 0.2332,     Val Loss : 0.1621, Val AUC: 0.6651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.06it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 022, Loss : 0.2367,     Val Loss : 0.2009, Val AUC: 0.6687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.37it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 023, Loss : 0.2339,     Val Loss : 0.1980, Val AUC: 0.6753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.28it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 024, Loss : 0.2358,     Val Loss : 0.1642, Val AUC: 0.6695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.40it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 025, Loss : 0.2325,     Val Loss : 0.2219, Val AUC: 0.6610\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.34it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 026, Loss : 0.2333,     Val Loss : 0.2508, Val AUC: 0.6793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.13it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 027, Loss : 0.2380,     Val Loss : 0.1624, Val AUC: 0.6021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.89it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 16.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 028, Loss : 0.2353,     Val Loss : 0.2316, Val AUC: 0.6650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.97it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 029, Loss : 0.2360,     Val Loss : 0.1837, Val AUC: 0.6591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  7.01it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 19.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 030, Loss : 0.2384,     Val Loss : 0.2839, Val AUC: 0.6514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.12it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 031, Loss : 0.2376,     Val Loss : 0.1581, Val AUC: 0.6587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.44it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 032, Loss : 0.2339,     Val Loss : 0.2320, Val AUC: 0.6620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.29it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 033, Loss : 0.2356,     Val Loss : 0.1830, Val AUC: 0.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.35it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 034, Loss : 0.2368,     Val Loss : 0.2024, Val AUC: 0.6567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:15<00:00,  7.50it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 035, Loss : 0.2345,     Val Loss : 0.3482, Val AUC: 0.6508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:15<00:00,  7.55it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 036, Loss : 0.2347,     Val Loss : 0.1829, Val AUC: 0.6516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.44it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 037, Loss : 0.2369,     Val Loss : 0.2588, Val AUC: 0.6540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:15<00:00,  7.52it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 038, Loss : 0.2359,     Val Loss : 0.2213, Val AUC: 0.6542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:15<00:00,  7.53it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 039, Loss : 0.2341,     Val Loss : 0.3476, Val AUC: 0.6679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.47it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 040, Loss : 0.2352,     Val Loss : 0.2558, Val AUC: 0.6706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.48it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 041, Loss : 0.2372,     Val Loss : 0.2124, Val AUC: 0.6745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  7.01it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 19.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 042, Loss : 0.2351,     Val Loss : 0.2335, Val AUC: 0.6716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.59it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 043, Loss : 0.2361,     Val Loss : 0.1765, Val AUC: 0.6654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.26it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 044, Loss : 0.2358,     Val Loss : 0.2281, Val AUC: 0.6537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.12it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 045, Loss : 0.2350,     Val Loss : 0.1518, Val AUC: 0.6472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  7.03it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 046, Loss : 0.2340,     Val Loss : 0.2664, Val AUC: 0.6453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  7.03it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 047, Loss : 0.2350,     Val Loss : 0.2464, Val AUC: 0.6655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.97it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 048, Loss : 0.2325,     Val Loss : 0.2084, Val AUC: 0.6624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.18it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 049, Loss : 0.2324,     Val Loss : 0.2639, Val AUC: 0.6672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.42it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 050, Loss : 0.2323,     Val Loss : 0.2412, Val AUC: 0.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:15<00:00,  7.52it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 051, Loss : 0.2312,     Val Loss : 0.1959, Val AUC: 0.6789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.45it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 052, Loss : 0.2324,     Val Loss : 0.2577, Val AUC: 0.6622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.49it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 053, Loss : 0.2331,     Val Loss : 0.2378, Val AUC: 0.6941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.45it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 054, Loss : 0.2292,     Val Loss : 0.2230, Val AUC: 0.6822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:15<00:00,  7.52it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 055, Loss : 0.2309,     Val Loss : 0.2556, Val AUC: 0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.96it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 18.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 056, Loss : 0.2315,     Val Loss : 0.2261, Val AUC: 0.6894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.62it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 18.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 057, Loss : 0.2333,     Val Loss : 0.2800, Val AUC: 0.6664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.36it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 058, Loss : 0.2310,     Val Loss : 0.1929, Val AUC: 0.6826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.39it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 059, Loss : 0.2327,     Val Loss : 0.2569, Val AUC: 0.6658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.50it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 23.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 060, Loss : 0.2287,     Val Loss : 0.2806, Val AUC: 0.6668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.44it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 061, Loss : 0.2302,     Val Loss : 0.2990, Val AUC: 0.6541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.15it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 062, Loss : 0.2306,     Val Loss : 0.1873, Val AUC: 0.6237\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.17it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 063, Loss : 0.2338,     Val Loss : 0.1940, Val AUC: 0.5802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.19it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 064, Loss : 0.2319,     Val Loss : 0.2460, Val AUC: 0.5997\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.39it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 065, Loss : 0.2326,     Val Loss : 0.3398, Val AUC: 0.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.37it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 066, Loss : 0.2358,     Val Loss : 0.2509, Val AUC: 0.6447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.43it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 067, Loss : 0.2305,     Val Loss : 0.2210, Val AUC: 0.6578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.41it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 068, Loss : 0.2305,     Val Loss : 0.2449, Val AUC: 0.6210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.38it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 069, Loss : 0.2358,     Val Loss : 0.2367, Val AUC: 0.6429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.89it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 19.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 070, Loss : 0.2342,     Val Loss : 0.2118, Val AUC: 0.6569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.35it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 071, Loss : 0.2307,     Val Loss : 0.2502, Val AUC: 0.6451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.28it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 072, Loss : 0.2314,     Val Loss : 0.2310, Val AUC: 0.6317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.33it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 073, Loss : 0.2319,     Val Loss : 0.3444, Val AUC: 0.5919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.39it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 074, Loss : 0.2331,     Val Loss : 0.2468, Val AUC: 0.6436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:15<00:00,  7.58it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 075, Loss : 0.2344,     Val Loss : 0.2757, Val AUC: 0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.15it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 076, Loss : 0.2333,     Val Loss : 0.2582, Val AUC: 0.6564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.31it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 077, Loss : 0.2309,     Val Loss : 0.2398, Val AUC: 0.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.41it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 078, Loss : 0.2346,     Val Loss : 0.2473, Val AUC: 0.6463\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.21it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 079, Loss : 0.2292,     Val Loss : 0.1846, Val AUC: 0.6151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.13it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 080, Loss : 0.2329,     Val Loss : 0.2673, Val AUC: 0.6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.39it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 081, Loss : 0.2352,     Val Loss : 0.3406, Val AUC: 0.6682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.36it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 082, Loss : 0.2309,     Val Loss : 0.2371, Val AUC: 0.6482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.42it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 083, Loss : 0.2313,     Val Loss : 0.1876, Val AUC: 0.6363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.65it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 18.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 084, Loss : 0.2333,     Val Loss : 0.1771, Val AUC: 0.6634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.51it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 085, Loss : 0.2369,     Val Loss : 0.2343, Val AUC: 0.6483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.18it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 086, Loss : 0.2328,     Val Loss : 0.2415, Val AUC: 0.6753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.30it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 087, Loss : 0.2321,     Val Loss : 0.2211, Val AUC: 0.5625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.45it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 088, Loss : 0.2325,     Val Loss : 0.2180, Val AUC: 0.6003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.32it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 089, Loss : 0.2326,     Val Loss : 0.1938, Val AUC: 0.6078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.20it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 090, Loss : 0.2342,     Val Loss : 0.2492, Val AUC: 0.6091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.47it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 091, Loss : 0.2313,     Val Loss : 0.2026, Val AUC: 0.6444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.35it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 092, Loss : 0.2350,     Val Loss : 0.2624, Val AUC: 0.5525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.28it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 093, Loss : 0.2349,     Val Loss : 0.2130, Val AUC: 0.6006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.11it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 094, Loss : 0.2306,     Val Loss : 0.2446, Val AUC: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.45it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 22.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 095, Loss : 0.2323,     Val Loss : 0.2597, Val AUC: 0.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.39it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 096, Loss : 0.2306,     Val Loss : 0.2778, Val AUC: 0.5753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:16<00:00,  7.35it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 20.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 097, Loss : 0.2321,     Val Loss : 0.2324, Val AUC: 0.6044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:18<00:00,  6.54it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 18.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 098, Loss : 0.2350,     Val Loss : 0.2891, Val AUC: 0.6042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train: 100%|██████████| 120/120 [00:17<00:00,  6.74it/s]\n",
      "test:val: 100%|██████████| 14/14 [00:00<00:00, 21.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 099, Loss : 0.2319,     Val Loss : 0.2540, Val AUC: 0.6227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "Best_Val_fromloss=1e10\n",
    "Final_Test_AUC_fromloss=0\n",
    "Final_Test_AP_fromloss=0\n",
    "\n",
    "Best_Val_fromAUC=0\n",
    "Final_Test_AUC_fromAUC=0\n",
    "Final_Test_AP_fromAUC=0\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    loss_epoch = train(train_loader,epoch)\n",
    "    val_auc, val_ap, val_loss,_ = test(val_loader,data_type='val')\n",
    "    #test_auc,test_ap,_,_ = test(test_loader,data_type='test')\n",
    "    if val_loss < Best_Val_fromloss:\n",
    "        Best_Val_fromloss = val_loss\n",
    "        #Final_Test_AUC_fromloss = test_auc\n",
    "        #Final_Test_AP_fromloss = test_ap\n",
    "\n",
    "    if val_auc > Best_Val_fromAUC:\n",
    "        Best_Val_fromAUC = val_auc\n",
    "        #Final_Test_AUC_fromAUC = test_auc\n",
    "        #Final_Test_AP_fromAUC = test_ap\n",
    "    print(f'Epoch: {epoch:03d}, Loss : {loss_epoch:.4f},\\\n",
    "     Val Loss : {val_loss:.4f}, Val AUC: {val_auc:.4f}')\n",
    "    '''\n",
    "    print(f'Epoch: {epoch:03d}, Loss : {loss_epoch:.4f},\\\n",
    "     Val Loss : {val_loss:.4f}, Val AUC: {val_auc:.4f},\\\n",
    "      Test AUC: {test_auc:.4f}, Picked AUC:{Final_Test_AUC_fromAUC:.4f}')\n",
    "print(f'From loss: Final Test AUC: {Final_Test_AUC_fromloss:.4f}, Final Test AP: {Final_Test_AP_fromloss:.4f}')\n",
    "print(f'From AUC: Final Test AUC: {Final_Test_AUC_fromAUC:.4f}, Final Test AP: {Final_Test_AP_fromAUC:.4f}')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8e5ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "scores = torch.tensor([])\n",
    "labels = torch.tensor([])\n",
    "loss_total=0\n",
    "with torch.no_grad():\n",
    "    for data in tqdm(test_loader,desc='test:'):  # Iterate in batches over the training/test dataset.\n",
    "        data = data.to(device)\n",
    "        out = model(data.x, data.edge_index, data.edge_mask, data.batch, data.z)\n",
    "        out = out.cpu().clone().detach()\n",
    "        scores = torch.cat((scores,out),dim = 0)\n",
    "    scores = scores.cpu().clone().detach().numpy()\n",
    "scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eda224",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c91cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upload['prob'] = scores.flatten().astype('float64')\n",
    "Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e0d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "Upload.to_csv('output/1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7433dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
